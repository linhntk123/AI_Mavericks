{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "MNCJwQ8THI_r"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler, random_split\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from torch.utils.data import Subset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "iG5ohW5SIazD"
      },
      "outputs": [],
      "source": [
        "# Set device to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "YKHP4eq5MsJv"
      },
      "outputs": [],
      "source": [
        "# Define transformations for the KMNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert images to tensors\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize pixel values\n",
        "])\n",
        "\n",
        "# Load the KMNIST dataset for training and testing\n",
        "train_dataset = datasets.KMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.KMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# train_dataset, _ = random_split(train_dataset, [1000, len(train_dataset) - 1000])\n",
        "# test_dataset, _ = random_split(test_dataset, [200, len(test_dataset) - 200])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torchvision.datasets.mnist.KMNIST"
            ]
          },
          "execution_count": 129,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "kcWN2l-ROB7x"
      },
      "outputs": [],
      "source": [
        "# Define the neural network architecture\n",
        "class SimpleANN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleANN, self).__init__()\n",
        "\n",
        "        # Define the layers of the neural network\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Flatten(),  # Flatten the input from 28x28 to 784\n",
        "            nn.Linear(28*28, 128),  # First fully connected layer (784 -> 512)\n",
        "            nn.BatchNorm1d(128),  # Batch normalization\n",
        "            nn.ReLU(),  # ReLU activation function\n",
        "            nn.Dropout(0.2),  # Dropout to avoid overfitting\n",
        "\n",
        "            nn.Linear(128, 64),  # Third fully connected layer (256 -> 128)\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(64, 10),  # Output layer (128 -> 10)\n",
        "            nn.BatchNorm1d(10)\n",
        "        )\n",
        "        self.init_weights()  # Initialize weights\n",
        "\n",
        "    def init_weights(self):\n",
        "        # Initialize weights uniformly and set biases to zero\n",
        "        for module in self.model.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                nn.init.uniform_(module.weight, a=-0.1, b=0.1)\n",
        "                nn.init.constant_(module.bias, 0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through the network\n",
        "        x = self.model(x)\n",
        "        x = F.softmax(x, dim=1)  # Apply softmax to output layer\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "kB-RpsRgUf5E"
      },
      "outputs": [],
      "source": [
        "# Function to train and evaluate the model using cross-validation\n",
        "def train_and_evaluate(optimizer_name, learning_rate, batch_size):\n",
        "    # Perform K-Fold cross-validation with 3 splits\n",
        "    kf = KFold(n_splits=3)\n",
        "    accuracy_scores = []\n",
        "    loss_scores = []\n",
        "    train_times = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(train_dataset)):\n",
        "        print(f\"Fold {fold + 1}/{kf.n_splits}\")\n",
        "\n",
        "        # Create data loaders for training and validation\n",
        "        train_sampler = SubsetRandomSampler(train_idx)\n",
        "        val_sampler = SubsetRandomSampler(val_idx)\n",
        "\n",
        "        train_loader_cv = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
        "        val_loader_cv = DataLoader(train_dataset, batch_size=batch_size, sampler=val_sampler)\n",
        "\n",
        "        # Instantiate the model and move it to the device (GPU/CPU)\n",
        "        model = SimpleANN().to(device)\n",
        "\n",
        "        # Define optimizer based on input string\n",
        "        if optimizer_name == 'adam':\n",
        "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "        elif optimizer_name == 'rmsprop':\n",
        "            optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "        elif optimizer_name == 'adamw':\n",
        "            optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "        # Use Mean Squared Error (MSE) as the loss function\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Training loop\n",
        "        model.train()  # Set the model to training mode\n",
        "        start_time = time.time()\n",
        "        for epoch in range(10):  # Train for 10 epochs\n",
        "            running_loss = 0.0\n",
        "            train_correct = 0\n",
        "\n",
        "            # Iterate over training batches\n",
        "            for batch_idx, (inputs, targets) in enumerate(train_loader_cv):\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                one_hot_targets = F.one_hot(targets, num_classes=10).float()  # One-hot encode targets\n",
        "\n",
        "                optimizer.zero_grad()  # Zero the parameter gradients\n",
        "                outputs = model(inputs)  # Forward pass\n",
        "                loss = criterion(outputs, one_hot_targets)  # Calculate loss\n",
        "                loss.backward()  # Backpropagation\n",
        "                optimizer.step()  # Update weights\n",
        "\n",
        "                running_loss += loss.item()  # Accumulate loss\n",
        "                _, predicted = torch.max(outputs, 1)  # Get predicted class\n",
        "                train_correct += (predicted == targets).sum().item()\n",
        "\n",
        "            # Calculate training accuracy\n",
        "            train_loss = running_loss / len(train_loader_cv)\n",
        "            train_accuracy = train_correct / len(train_sampler)\n",
        "\n",
        "            # Validation loop\n",
        "            model.eval()  # Set model to evaluation mode\n",
        "            val_loss = 0.0\n",
        "            val_correct = 0\n",
        "            with torch.no_grad():  # Disable gradient calculation for validation\n",
        "                for batch_idx, (inputs, targets) in enumerate(val_loader_cv):\n",
        "                    inputs, targets = inputs.to(device), targets.to(device)\n",
        "                    one_hot_targets = F.one_hot(targets, num_classes=10).float()\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, one_hot_targets)\n",
        "                    val_loss += loss.item()\n",
        "                    _, predicted = torch.max(outputs, 1)\n",
        "                    val_correct += (predicted == targets).sum().item()\n",
        "\n",
        "            # Calculate validation accuracy\n",
        "            val_loss /= len(val_loader_cv)\n",
        "            val_accuracy = val_correct / len(val_sampler)\n",
        "\n",
        "            print(f\"  Epoch {epoch + 1}/10 - train_loss: {train_loss:.4f} - train_acc: {train_accuracy:.4f} - val_loss: {val_loss:.4f} - val_acc: {val_accuracy:.4f}\")\n",
        "        # End of fold, record training time\n",
        "        train_time = time.time() - start_time\n",
        "\n",
        "        accuracy_scores.append(val_accuracy)  # Append accuracy for this fold\n",
        "        loss_scores.append(val_loss)\n",
        "        train_times.append(train_time)\n",
        "\n",
        "    # Return average accuracy across folds\n",
        "    return np.mean(accuracy_scores), np.mean(loss_scores), np.mean(train_times)\n",
        "# List to store results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "pl0nJmPwUxuS"
      },
      "outputs": [],
      "source": [
        "# # Define hyperparameters for tuning\n",
        "learning_rates = [0.1, 0.01, 0.001]\n",
        "batch_sizes = [32, 64, 128]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTrhO1m5U5Pm",
        "outputId": "504c1c73-cb33-43b7-9684-fcd06ae824a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<---------------- Testing adam with learning rate 0.1 and batch size 32 ---------------->\n",
            "Fold 1/3\n",
            "  Epoch 1/10 - train_loss: 1.7547 - train_acc: 0.7108 - val_loss: 1.6327 - val_acc: 0.8299\n",
            "  Epoch 2/10 - train_loss: 1.7781 - train_acc: 0.6818 - val_loss: 1.7690 - val_acc: 0.6920\n",
            "  Epoch 3/10 - train_loss: 1.9209 - train_acc: 0.5400 - val_loss: 2.1697 - val_acc: 0.2914\n",
            "  Epoch 4/10 - train_loss: 2.1101 - train_acc: 0.3510 - val_loss: 2.0967 - val_acc: 0.3644\n",
            "  Epoch 5/10 - train_loss: 2.2991 - train_acc: 0.1620 - val_loss: 2.3616 - val_acc: 0.0996\n",
            "  Epoch 6/10 - train_loss: 2.3609 - train_acc: 0.1003 - val_loss: 2.3616 - val_acc: 0.0996\n",
            "  Epoch 7/10 - train_loss: 2.3609 - train_acc: 0.1003 - val_loss: 2.3616 - val_acc: 0.0996\n",
            "  Epoch 8/10 - train_loss: 2.3609 - train_acc: 0.1003 - val_loss: 2.3616 - val_acc: 0.0996\n",
            "  Epoch 9/10 - train_loss: 2.3609 - train_acc: 0.1003 - val_loss: 2.3616 - val_acc: 0.0996\n",
            "  Epoch 10/10 - train_loss: 2.3609 - train_acc: 0.1003 - val_loss: 2.3616 - val_acc: 0.0996\n",
            "Fold 2/3\n",
            "  Epoch 1/10 - train_loss: 1.7573 - train_acc: 0.7081 - val_loss: 1.6317 - val_acc: 0.8306\n",
            "  Epoch 2/10 - train_loss: 1.8573 - train_acc: 0.6029 - val_loss: 2.0595 - val_acc: 0.4014\n",
            "  Epoch 3/10 - train_loss: 2.1438 - train_acc: 0.3173 - val_loss: 2.2801 - val_acc: 0.1810\n",
            "  Epoch 4/10 - train_loss: 2.3194 - train_acc: 0.1417 - val_loss: 2.2893 - val_acc: 0.1718\n",
            "  Epoch 5/10 - train_loss: 2.2549 - train_acc: 0.2062 - val_loss: 2.2634 - val_acc: 0.1978\n",
            "  Epoch 6/10 - train_loss: 2.3095 - train_acc: 0.1517 - val_loss: 2.3303 - val_acc: 0.1309\n",
            "  Epoch 7/10 - train_loss: 2.3229 - train_acc: 0.1382 - val_loss: 2.3589 - val_acc: 0.1023\n",
            "  Epoch 8/10 - train_loss: 2.3585 - train_acc: 0.1027 - val_loss: 2.3529 - val_acc: 0.1082\n",
            "  Epoch 9/10 - train_loss: 2.3400 - train_acc: 0.1212 - val_loss: 2.3467 - val_acc: 0.1144\n",
            "  Epoch 10/10 - train_loss: 2.3313 - train_acc: 0.1298 - val_loss: 2.3566 - val_acc: 0.1046\n",
            "Fold 3/3\n",
            "  Epoch 1/10 - train_loss: 1.7532 - train_acc: 0.7122 - val_loss: 1.6248 - val_acc: 0.8373\n",
            "  Epoch 2/10 - train_loss: 1.8663 - train_acc: 0.5944 - val_loss: 1.9815 - val_acc: 0.4797\n",
            "  Epoch 3/10 - train_loss: 2.0474 - train_acc: 0.4137 - val_loss: 2.1577 - val_acc: 0.3034\n",
            "  Epoch 4/10 - train_loss: 2.1865 - train_acc: 0.2746 - val_loss: 2.3326 - val_acc: 0.1285\n",
            "  Epoch 5/10 - train_loss: 2.2663 - train_acc: 0.1948 - val_loss: 2.1484 - val_acc: 0.3127\n",
            "  Epoch 6/10 - train_loss: 2.1778 - train_acc: 0.2834 - val_loss: 2.2040 - val_acc: 0.2571\n",
            "  Epoch 7/10 - train_loss: 2.2207 - train_acc: 0.2405 - val_loss: 2.2321 - val_acc: 0.2291\n",
            "  Epoch 8/10 - train_loss: 2.3031 - train_acc: 0.1581 - val_loss: 2.1995 - val_acc: 0.2617\n",
            "  Epoch 9/10 - train_loss: 2.2228 - train_acc: 0.2384 - val_loss: 2.2647 - val_acc: 0.1964\n",
            "  Epoch 10/10 - train_loss: 2.1858 - train_acc: 0.2754 - val_loss: 2.1346 - val_acc: 0.3266\n",
            "<---------------- Testing adam with learning rate 0.1 and batch size 64 ---------------->\n",
            "Fold 1/3\n",
            "  Epoch 1/10 - train_loss: 1.7183 - train_acc: 0.7519 - val_loss: 1.6284 - val_acc: 0.8349\n",
            "  Epoch 2/10 - train_loss: 1.9089 - train_acc: 0.5517 - val_loss: 2.2456 - val_acc: 0.2151\n",
            "  Epoch 3/10 - train_loss: 2.2132 - train_acc: 0.2479 - val_loss: 2.2774 - val_acc: 0.1837\n",
            "  Epoch 4/10 - train_loss: 2.2694 - train_acc: 0.1917 - val_loss: 2.1528 - val_acc: 0.3084\n",
            "  Epoch 5/10 - train_loss: 2.3007 - train_acc: 0.1604 - val_loss: 2.3414 - val_acc: 0.1197\n",
            "  Epoch 6/10 - train_loss: 2.3180 - train_acc: 0.1431 - val_loss: 2.3144 - val_acc: 0.1467\n",
            "  Epoch 7/10 - train_loss: 2.2979 - train_acc: 0.1632 - val_loss: 2.2947 - val_acc: 0.1665\n",
            "  Epoch 8/10 - train_loss: 2.2909 - train_acc: 0.1702 - val_loss: 2.2908 - val_acc: 0.1706\n",
            "  Epoch 9/10 - train_loss: 2.2925 - train_acc: 0.1686 - val_loss: 2.3613 - val_acc: 0.0998\n",
            "  Epoch 10/10 - train_loss: 2.3605 - train_acc: 0.1007 - val_loss: 2.3615 - val_acc: 0.0998\n",
            "Fold 2/3\n",
            "  Epoch 1/10 - train_loss: 1.7025 - train_acc: 0.7668 - val_loss: 1.6057 - val_acc: 0.8571\n",
            "  Epoch 2/10 - train_loss: 1.8689 - train_acc: 0.5911 - val_loss: 2.1676 - val_acc: 0.2936\n",
            "  Epoch 3/10 - train_loss: 2.2262 - train_acc: 0.2349 - val_loss: 2.3605 - val_acc: 0.1008\n",
            "  Epoch 4/10 - train_loss: 2.3615 - train_acc: 0.0997 - val_loss: 2.3606 - val_acc: 0.1007\n",
            "  Epoch 5/10 - train_loss: 2.3615 - train_acc: 0.0997 - val_loss: 2.3605 - val_acc: 0.1007\n",
            "  Epoch 6/10 - train_loss: 2.3615 - train_acc: 0.0997 - val_loss: 2.3603 - val_acc: 0.1007\n",
            "  Epoch 7/10 - train_loss: 2.3615 - train_acc: 0.0997 - val_loss: 2.3604 - val_acc: 0.1007\n",
            "  Epoch 8/10 - train_loss: 2.3615 - train_acc: 0.0997 - val_loss: 2.3604 - val_acc: 0.1007\n",
            "  Epoch 9/10 - train_loss: 2.3615 - train_acc: 0.0997 - val_loss: 2.3605 - val_acc: 0.1007\n",
            "  Epoch 10/10 - train_loss: 2.3615 - train_acc: 0.0997 - val_loss: 2.3605 - val_acc: 0.1007\n",
            "Fold 3/3\n",
            "  Epoch 1/10 - train_loss: 1.7068 - train_acc: 0.7621 - val_loss: 1.6017 - val_acc: 0.8599\n",
            "  Epoch 2/10 - train_loss: 1.8767 - train_acc: 0.5834 - val_loss: 2.2266 - val_acc: 0.2344\n",
            "  Epoch 3/10 - train_loss: 2.3453 - train_acc: 0.1159 - val_loss: 2.3193 - val_acc: 0.1418\n",
            "  Epoch 4/10 - train_loss: 2.3484 - train_acc: 0.1127 - val_loss: 2.3557 - val_acc: 0.1056\n",
            "  Epoch 5/10 - train_loss: 2.3448 - train_acc: 0.1163 - val_loss: 2.3179 - val_acc: 0.1434\n",
            "  Epoch 6/10 - train_loss: 2.3357 - train_acc: 0.1255 - val_loss: 2.3612 - val_acc: 0.0999\n",
            "  Epoch 7/10 - train_loss: 2.3603 - train_acc: 0.1008 - val_loss: 2.3610 - val_acc: 0.1003\n",
            "  Epoch 8/10 - train_loss: 2.3604 - train_acc: 0.1008 - val_loss: 2.3614 - val_acc: 0.0996\n",
            "  Epoch 9/10 - train_loss: 2.3609 - train_acc: 0.1003 - val_loss: 2.3617 - val_acc: 0.0996\n",
            "  Epoch 10/10 - train_loss: 2.3609 - train_acc: 0.1003 - val_loss: 2.3616 - val_acc: 0.0996\n",
            "<---------------- Testing adam with learning rate 0.1 and batch size 128 ---------------->\n",
            "Fold 1/3\n",
            "  Epoch 1/10 - train_loss: 1.6985 - train_acc: 0.7750 - val_loss: 1.6240 - val_acc: 0.8397\n",
            "  Epoch 2/10 - train_loss: 1.7462 - train_acc: 0.7143 - val_loss: 1.8168 - val_acc: 0.6439\n",
            "  Epoch 3/10 - train_loss: 2.2041 - train_acc: 0.2570 - val_loss: 2.2797 - val_acc: 0.1814\n",
            "  Epoch 4/10 - train_loss: 2.1999 - train_acc: 0.2614 - val_loss: 2.2764 - val_acc: 0.1850\n",
            "  Epoch 5/10 - train_loss: 2.2763 - train_acc: 0.1847 - val_loss: 2.1843 - val_acc: 0.2772\n",
            "  Epoch 6/10 - train_loss: 2.3263 - train_acc: 0.1348 - val_loss: 2.3598 - val_acc: 0.1014\n",
            "  Epoch 7/10 - train_loss: 2.3594 - train_acc: 0.1018 - val_loss: 2.3595 - val_acc: 0.1012\n",
            "  Epoch 8/10 - train_loss: 2.3593 - train_acc: 0.1018 - val_loss: 2.3598 - val_acc: 0.1012\n",
            "  Epoch 9/10 - train_loss: 2.3594 - train_acc: 0.1018 - val_loss: 2.3597 - val_acc: 0.1012\n",
            "  Epoch 10/10 - train_loss: 2.3592 - train_acc: 0.1018 - val_loss: 2.3598 - val_acc: 0.1012\n",
            "Fold 2/3\n",
            "  Epoch 1/10 - train_loss: 1.6813 - train_acc: 0.7948 - val_loss: 1.5900 - val_acc: 0.8735\n",
            "  Epoch 2/10 - train_loss: 1.8441 - train_acc: 0.6170 - val_loss: 2.1763 - val_acc: 0.2848\n",
            "  Epoch 3/10 - train_loss: 2.3582 - train_acc: 0.1030 - val_loss: 2.3605 - val_acc: 0.1010\n",
            "  Epoch 4/10 - train_loss: 2.3605 - train_acc: 0.1007 - val_loss: 2.3599 - val_acc: 0.1013\n",
            "  Epoch 5/10 - train_loss: 2.3605 - train_acc: 0.1008 - val_loss: 2.3601 - val_acc: 0.1013\n",
            "  Epoch 6/10 - train_loss: 2.3602 - train_acc: 0.1009 - val_loss: 2.3597 - val_acc: 0.1013\n",
            "  Epoch 7/10 - train_loss: 2.3546 - train_acc: 0.1067 - val_loss: 2.3615 - val_acc: 0.0999\n",
            "  Epoch 8/10 - train_loss: 2.3612 - train_acc: 0.1000 - val_loss: 2.3609 - val_acc: 0.0999\n",
            "  Epoch 9/10 - train_loss: 2.3611 - train_acc: 0.1000 - val_loss: 2.3612 - val_acc: 0.0999\n",
            "  Epoch 10/10 - train_loss: 2.3612 - train_acc: 0.1000 - val_loss: 2.3611 - val_acc: 0.0999\n",
            "Fold 3/3\n",
            "  Epoch 1/10 - train_loss: 1.6968 - train_acc: 0.7774 - val_loss: 1.6103 - val_acc: 0.8526\n",
            "  Epoch 2/10 - train_loss: 1.7516 - train_acc: 0.7092 - val_loss: 1.9154 - val_acc: 0.5457\n",
            "  Epoch 3/10 - train_loss: 2.1561 - train_acc: 0.3052 - val_loss: 2.2679 - val_acc: 0.1933\n",
            "  Epoch 4/10 - train_loss: 2.1846 - train_acc: 0.2763 - val_loss: 2.2102 - val_acc: 0.2507\n",
            "  Epoch 5/10 - train_loss: 2.3004 - train_acc: 0.1607 - val_loss: 2.3042 - val_acc: 0.1567\n",
            "  Epoch 6/10 - train_loss: 2.2917 - train_acc: 0.1694 - val_loss: 2.2712 - val_acc: 0.1897\n",
            "  Epoch 7/10 - train_loss: 2.2784 - train_acc: 0.1827 - val_loss: 2.2736 - val_acc: 0.1877\n",
            "  Epoch 8/10 - train_loss: 2.2960 - train_acc: 0.1652 - val_loss: 2.3523 - val_acc: 0.1090\n",
            "  Epoch 9/10 - train_loss: 2.3417 - train_acc: 0.1195 - val_loss: 2.3289 - val_acc: 0.1320\n",
            "  Epoch 10/10 - train_loss: 2.3269 - train_acc: 0.1341 - val_loss: 2.2918 - val_acc: 0.1693\n",
            "<---------------- Testing adam with learning rate 0.01 and batch size 32 ---------------->\n",
            "Fold 1/3\n",
            "  Epoch 1/10 - train_loss: 1.7118 - train_acc: 0.7842 - val_loss: 1.5928 - val_acc: 0.8790\n",
            "  Epoch 2/10 - train_loss: 1.6341 - train_acc: 0.8274 - val_loss: 1.6169 - val_acc: 0.8434\n",
            "  Epoch 3/10 - train_loss: 1.6379 - train_acc: 0.8226 - val_loss: 1.6371 - val_acc: 0.8236\n",
            "  Epoch 4/10 - train_loss: 1.6687 - train_acc: 0.7920 - val_loss: 1.7338 - val_acc: 0.7272\n",
            "  Epoch 5/10 - train_loss: 1.6828 - train_acc: 0.7781 - val_loss: 1.6673 - val_acc: 0.7933\n",
            "  Epoch 6/10 - train_loss: 1.7191 - train_acc: 0.7421 - val_loss: 1.7538 - val_acc: 0.7075\n",
            "  Epoch 7/10 - train_loss: 1.7380 - train_acc: 0.7230 - val_loss: 1.7903 - val_acc: 0.6707\n",
            "  Epoch 8/10 - train_loss: 1.7678 - train_acc: 0.6932 - val_loss: 1.7814 - val_acc: 0.6798\n",
            "  Epoch 9/10 - train_loss: 1.7977 - train_acc: 0.6634 - val_loss: 1.7355 - val_acc: 0.7256\n",
            "  Epoch 10/10 - train_loss: 1.7529 - train_acc: 0.7082 - val_loss: 1.7086 - val_acc: 0.7526\n",
            "Fold 2/3\n",
            "  Epoch 1/10 - train_loss: 1.7141 - train_acc: 0.7809 - val_loss: 1.5888 - val_acc: 0.8824\n",
            "  Epoch 2/10 - train_loss: 1.6530 - train_acc: 0.8078 - val_loss: 1.6480 - val_acc: 0.8118\n",
            "  Epoch 3/10 - train_loss: 1.6656 - train_acc: 0.7949 - val_loss: 1.6474 - val_acc: 0.8135\n",
            "  Epoch 4/10 - train_loss: 1.6670 - train_acc: 0.7936 - val_loss: 1.6832 - val_acc: 0.7779\n",
            "  Epoch 5/10 - train_loss: 1.6788 - train_acc: 0.7818 - val_loss: 1.7509 - val_acc: 0.7101\n",
            "  Epoch 6/10 - train_loss: 1.6956 - train_acc: 0.7653 - val_loss: 1.6552 - val_acc: 0.8058\n",
            "  Epoch 7/10 - train_loss: 1.7813 - train_acc: 0.6796 - val_loss: 1.7523 - val_acc: 0.7087\n",
            "  Epoch 8/10 - train_loss: 1.7541 - train_acc: 0.7070 - val_loss: 1.7457 - val_acc: 0.7154\n",
            "  Epoch 9/10 - train_loss: 1.7488 - train_acc: 0.7123 - val_loss: 1.7614 - val_acc: 0.6997\n",
            "  Epoch 10/10 - train_loss: 1.8138 - train_acc: 0.6474 - val_loss: 1.9334 - val_acc: 0.5277\n",
            "Fold 3/3\n",
            "  Epoch 1/10 - train_loss: 1.7144 - train_acc: 0.7807 - val_loss: 1.5826 - val_acc: 0.8868\n",
            "  Epoch 2/10 - train_loss: 1.6501 - train_acc: 0.8109 - val_loss: 1.6752 - val_acc: 0.7850\n",
            "  Epoch 3/10 - train_loss: 1.6505 - train_acc: 0.8099 - val_loss: 1.6548 - val_acc: 0.8059\n",
            "  Epoch 4/10 - train_loss: 1.6630 - train_acc: 0.7976 - val_loss: 1.6440 - val_acc: 0.8166\n",
            "  Epoch 5/10 - train_loss: 1.6908 - train_acc: 0.7701 - val_loss: 1.7350 - val_acc: 0.7257\n",
            "  Epoch 6/10 - train_loss: 1.7273 - train_acc: 0.7337 - val_loss: 1.7092 - val_acc: 0.7518\n",
            "  Epoch 7/10 - train_loss: 1.7098 - train_acc: 0.7513 - val_loss: 1.7281 - val_acc: 0.7329\n",
            "  Epoch 8/10 - train_loss: 1.7353 - train_acc: 0.7257 - val_loss: 1.7431 - val_acc: 0.7180\n",
            "  Epoch 9/10 - train_loss: 1.7037 - train_acc: 0.7574 - val_loss: 1.7216 - val_acc: 0.7395\n",
            "  Epoch 10/10 - train_loss: 1.7728 - train_acc: 0.6884 - val_loss: 1.7046 - val_acc: 0.7566\n",
            "<---------------- Testing adam with learning rate 0.01 and batch size 64 ---------------->\n",
            "Fold 1/3\n",
            "  Epoch 1/10 - train_loss: 1.6919 - train_acc: 0.8156 - val_loss: 1.5761 - val_acc: 0.8990\n",
            "  Epoch 2/10 - train_loss: 1.6306 - train_acc: 0.8306 - val_loss: 1.6786 - val_acc: 0.7817\n",
            "  Epoch 3/10 - train_loss: 1.6242 - train_acc: 0.8365 - val_loss: 1.6625 - val_acc: 0.7984\n",
            "  Epoch 4/10 - train_loss: 1.6544 - train_acc: 0.8061 - val_loss: 1.7064 - val_acc: 0.7540\n",
            "  Epoch 5/10 - train_loss: 1.6650 - train_acc: 0.7957 - val_loss: 1.7049 - val_acc: 0.7558\n",
            "  Epoch 6/10 - train_loss: 1.6523 - train_acc: 0.8085 - val_loss: 1.7412 - val_acc: 0.7196\n",
            "  Epoch 7/10 - train_loss: 1.6838 - train_acc: 0.7771 - val_loss: 1.7098 - val_acc: 0.7514\n",
            "  Epoch 8/10 - train_loss: 1.7225 - train_acc: 0.7386 - val_loss: 1.7861 - val_acc: 0.6751\n",
            "  Epoch 9/10 - train_loss: 1.7181 - train_acc: 0.7429 - val_loss: 1.6938 - val_acc: 0.7671\n",
            "  Epoch 10/10 - train_loss: 1.7067 - train_acc: 0.7544 - val_loss: 1.8439 - val_acc: 0.6172\n",
            "Fold 2/3\n",
            "  Epoch 1/10 - train_loss: 1.6937 - train_acc: 0.8157 - val_loss: 1.5756 - val_acc: 0.8984\n",
            "  Epoch 2/10 - train_loss: 1.6402 - train_acc: 0.8212 - val_loss: 1.6458 - val_acc: 0.8148\n",
            "  Epoch 3/10 - train_loss: 1.6234 - train_acc: 0.8368 - val_loss: 1.6282 - val_acc: 0.8322\n",
            "  Epoch 4/10 - train_loss: 1.6383 - train_acc: 0.8224 - val_loss: 1.6498 - val_acc: 0.8108\n",
            "  Epoch 5/10 - train_loss: 1.6466 - train_acc: 0.8143 - val_loss: 1.6629 - val_acc: 0.7976\n",
            "  Epoch 6/10 - train_loss: 1.6761 - train_acc: 0.7847 - val_loss: 1.6292 - val_acc: 0.8321\n",
            "  Epoch 7/10 - train_loss: 1.6894 - train_acc: 0.7714 - val_loss: 1.6504 - val_acc: 0.8106\n",
            "  Epoch 8/10 - train_loss: 1.6868 - train_acc: 0.7742 - val_loss: 1.7209 - val_acc: 0.7400\n",
            "  Epoch 9/10 - train_loss: 1.7494 - train_acc: 0.7116 - val_loss: 1.6914 - val_acc: 0.7699\n",
            "  Epoch 10/10 - train_loss: 1.7012 - train_acc: 0.7598 - val_loss: 1.7453 - val_acc: 0.7157\n",
            "Fold 3/3\n",
            "  Epoch 1/10 - train_loss: 1.6918 - train_acc: 0.8168 - val_loss: 1.5778 - val_acc: 0.8950\n",
            "  Epoch 2/10 - train_loss: 1.6399 - train_acc: 0.8211 - val_loss: 1.6448 - val_acc: 0.8158\n",
            "  Epoch 3/10 - train_loss: 1.6395 - train_acc: 0.8209 - val_loss: 1.6522 - val_acc: 0.8081\n",
            "  Epoch 4/10 - train_loss: 1.6369 - train_acc: 0.8238 - val_loss: 1.6570 - val_acc: 0.8037\n",
            "  Epoch 5/10 - train_loss: 1.6498 - train_acc: 0.8112 - val_loss: 1.6293 - val_acc: 0.8317\n",
            "  Epoch 6/10 - train_loss: 1.6758 - train_acc: 0.7852 - val_loss: 1.6430 - val_acc: 0.8181\n",
            "  Epoch 7/10 - train_loss: 1.6666 - train_acc: 0.7944 - val_loss: 1.6341 - val_acc: 0.8268\n",
            "  Epoch 8/10 - train_loss: 1.6779 - train_acc: 0.7830 - val_loss: 1.6964 - val_acc: 0.7644\n",
            "  Epoch 9/10 - train_loss: 1.6888 - train_acc: 0.7722 - val_loss: 1.6772 - val_acc: 0.7837\n",
            "  Epoch 10/10 - train_loss: 1.7093 - train_acc: 0.7517 - val_loss: 1.9041 - val_acc: 0.5572\n",
            "<---------------- Testing adam with learning rate 0.01 and batch size 128 ---------------->\n",
            "Fold 1/3\n",
            "  Epoch 1/10 - train_loss: 1.7027 - train_acc: 0.8277 - val_loss: 1.5779 - val_acc: 0.9009\n",
            "  Epoch 2/10 - train_loss: 1.6142 - train_acc: 0.8471 - val_loss: 1.6054 - val_acc: 0.8553\n",
            "  Epoch 3/10 - train_loss: 1.6065 - train_acc: 0.8543 - val_loss: 1.6249 - val_acc: 0.8367\n",
            "  Epoch 4/10 - train_loss: 1.6137 - train_acc: 0.8472 - val_loss: 1.6348 - val_acc: 0.8260\n",
            "  Epoch 5/10 - train_loss: 1.6086 - train_acc: 0.8521 - val_loss: 1.6600 - val_acc: 0.8007\n",
            "  Epoch 6/10 - train_loss: 1.6191 - train_acc: 0.8417 - val_loss: 1.6398 - val_acc: 0.8218\n",
            "  Epoch 7/10 - train_loss: 1.6316 - train_acc: 0.8296 - val_loss: 1.6963 - val_acc: 0.7638\n",
            "  Epoch 8/10 - train_loss: 1.6437 - train_acc: 0.8172 - val_loss: 1.6522 - val_acc: 0.8090\n",
            "  Epoch 9/10 - train_loss: 1.6374 - train_acc: 0.8237 - val_loss: 1.6854 - val_acc: 0.7758\n",
            "  Epoch 10/10 - train_loss: 1.6304 - train_acc: 0.8307 - val_loss: 1.6517 - val_acc: 0.8095\n",
            "Fold 2/3\n",
            "  Epoch 1/10 - train_loss: 1.7002 - train_acc: 0.8285 - val_loss: 1.5785 - val_acc: 0.9044\n",
            "  Epoch 2/10 - train_loss: 1.6223 - train_acc: 0.8388 - val_loss: 1.6115 - val_acc: 0.8490\n",
            "  Epoch 3/10 - train_loss: 1.6115 - train_acc: 0.8486 - val_loss: 1.6113 - val_acc: 0.8494\n",
            "  Epoch 4/10 - train_loss: 1.6066 - train_acc: 0.8541 - val_loss: 1.6142 - val_acc: 0.8470\n",
            "  Epoch 5/10 - train_loss: 1.6109 - train_acc: 0.8498 - val_loss: 1.6168 - val_acc: 0.8441\n",
            "  Epoch 6/10 - train_loss: 1.6335 - train_acc: 0.8272 - val_loss: 1.6462 - val_acc: 0.8151\n",
            "  Epoch 7/10 - train_loss: 1.6357 - train_acc: 0.8252 - val_loss: 1.6616 - val_acc: 0.7986\n",
            "  Epoch 8/10 - train_loss: 1.6600 - train_acc: 0.8010 - val_loss: 1.6582 - val_acc: 0.8030\n",
            "  Epoch 9/10 - train_loss: 1.6372 - train_acc: 0.8238 - val_loss: 1.6532 - val_acc: 0.8077\n",
            "  Epoch 10/10 - train_loss: 1.6477 - train_acc: 0.8136 - val_loss: 1.7893 - val_acc: 0.6712\n",
            "Fold 3/3\n",
            "  Epoch 1/10 - train_loss: 1.7030 - train_acc: 0.8251 - val_loss: 1.5868 - val_acc: 0.8982\n",
            "  Epoch 2/10 - train_loss: 1.6221 - train_acc: 0.8401 - val_loss: 1.6405 - val_acc: 0.8196\n",
            "  Epoch 3/10 - train_loss: 1.6069 - train_acc: 0.8539 - val_loss: 1.6118 - val_acc: 0.8488\n",
            "  Epoch 4/10 - train_loss: 1.6192 - train_acc: 0.8413 - val_loss: 1.6255 - val_acc: 0.8357\n",
            "  Epoch 5/10 - train_loss: 1.6148 - train_acc: 0.8457 - val_loss: 1.6228 - val_acc: 0.8379\n",
            "  Epoch 6/10 - train_loss: 1.6137 - train_acc: 0.8472 - val_loss: 1.6449 - val_acc: 0.8157\n",
            "  Epoch 7/10 - train_loss: 1.6312 - train_acc: 0.8296 - val_loss: 1.6253 - val_acc: 0.8353\n",
            "  Epoch 8/10 - train_loss: 1.6513 - train_acc: 0.8097 - val_loss: 1.6295 - val_acc: 0.8313\n",
            "  Epoch 9/10 - train_loss: 1.6614 - train_acc: 0.7994 - val_loss: 1.6300 - val_acc: 0.8310\n",
            "  Epoch 10/10 - train_loss: 1.6530 - train_acc: 0.8079 - val_loss: 1.6455 - val_acc: 0.8154\n",
            "<---------------- Testing adam with learning rate 0.001 and batch size 32 ---------------->\n",
            "Fold 1/3\n",
            "  Epoch 1/10 - train_loss: 1.8232 - train_acc: 0.7616 - val_loss: 1.6437 - val_acc: 0.8824\n",
            "  Epoch 2/10 - train_loss: 1.5846 - train_acc: 0.8817 - val_loss: 1.5649 - val_acc: 0.8974\n",
            "  Epoch 3/10 - train_loss: 1.5562 - train_acc: 0.9057 - val_loss: 1.5523 - val_acc: 0.9089\n",
            "  Epoch 4/10 - train_loss: 1.5421 - train_acc: 0.9200 - val_loss: 1.5532 - val_acc: 0.9081\n",
            "  Epoch 5/10 - train_loss: 1.5377 - train_acc: 0.9235 - val_loss: 1.5457 - val_acc: 0.9155\n",
            "  Epoch 6/10 - train_loss: 1.5320 - train_acc: 0.9297 - val_loss: 1.5440 - val_acc: 0.9172\n",
            "  Epoch 7/10 - train_loss: 1.5259 - train_acc: 0.9357 - val_loss: 1.5382 - val_acc: 0.9227\n",
            "  Epoch 8/10 - train_loss: 1.5259 - train_acc: 0.9351 - val_loss: 1.5524 - val_acc: 0.9087\n",
            "  Epoch 9/10 - train_loss: 1.5254 - train_acc: 0.9355 - val_loss: 1.5422 - val_acc: 0.9185\n",
            "  Epoch 10/10 - train_loss: 1.5231 - train_acc: 0.9385 - val_loss: 1.5374 - val_acc: 0.9233\n",
            "Fold 2/3\n",
            "  Epoch 1/10 - train_loss: 1.8251 - train_acc: 0.7582 - val_loss: 1.6532 - val_acc: 0.8755\n",
            "  Epoch 2/10 - train_loss: 1.5877 - train_acc: 0.8784 - val_loss: 1.6284 - val_acc: 0.8317\n",
            "  Epoch 3/10 - train_loss: 1.5571 - train_acc: 0.9050 - val_loss: 1.5744 - val_acc: 0.8869\n",
            "  Epoch 4/10 - train_loss: 1.5454 - train_acc: 0.9163 - val_loss: 1.5727 - val_acc: 0.8894\n",
            "  Epoch 5/10 - train_loss: 1.5387 - train_acc: 0.9228 - val_loss: 1.5523 - val_acc: 0.9088\n",
            "  Epoch 6/10 - train_loss: 1.5334 - train_acc: 0.9278 - val_loss: 1.5447 - val_acc: 0.9161\n",
            "  Epoch 7/10 - train_loss: 1.5304 - train_acc: 0.9308 - val_loss: 1.5560 - val_acc: 0.9046\n",
            "  Epoch 8/10 - train_loss: 1.5253 - train_acc: 0.9358 - val_loss: 1.5379 - val_acc: 0.9232\n",
            "  Epoch 9/10 - train_loss: 1.5243 - train_acc: 0.9370 - val_loss: 1.5381 - val_acc: 0.9233\n",
            "  Epoch 10/10 - train_loss: 1.5233 - train_acc: 0.9376 - val_loss: 1.5340 - val_acc: 0.9269\n",
            "Fold 3/3\n",
            "  Epoch 1/10 - train_loss: 1.8252 - train_acc: 0.7599 - val_loss: 1.6386 - val_acc: 0.8830\n",
            "  Epoch 2/10 - train_loss: 1.5873 - train_acc: 0.8789 - val_loss: 1.5762 - val_acc: 0.8864\n",
            "  Epoch 3/10 - train_loss: 1.5563 - train_acc: 0.9065 - val_loss: 1.5513 - val_acc: 0.9102\n",
            "  Epoch 4/10 - train_loss: 1.5472 - train_acc: 0.9140 - val_loss: 1.5608 - val_acc: 0.9004\n",
            "  Epoch 5/10 - train_loss: 1.5391 - train_acc: 0.9227 - val_loss: 1.5515 - val_acc: 0.9090\n",
            "  Epoch 6/10 - train_loss: 1.5341 - train_acc: 0.9274 - val_loss: 1.5541 - val_acc: 0.9071\n",
            "  Epoch 7/10 - train_loss: 1.5305 - train_acc: 0.9309 - val_loss: 1.5390 - val_acc: 0.9224\n",
            "  Epoch 8/10 - train_loss: 1.5295 - train_acc: 0.9314 - val_loss: 1.5343 - val_acc: 0.9269\n",
            "  Epoch 9/10 - train_loss: 1.5265 - train_acc: 0.9346 - val_loss: 1.5511 - val_acc: 0.9094\n",
            "  Epoch 10/10 - train_loss: 1.5241 - train_acc: 0.9367 - val_loss: 1.5426 - val_acc: 0.9179\n",
            "<---------------- Testing adam with learning rate 0.001 and batch size 64 ---------------->\n",
            "Fold 1/3\n",
            "  Epoch 1/10 - train_loss: 1.8442 - train_acc: 0.7710 - val_loss: 1.6818 - val_acc: 0.8770\n",
            "  Epoch 2/10 - train_loss: 1.5845 - train_acc: 0.8844 - val_loss: 1.5673 - val_acc: 0.8968\n",
            "  Epoch 3/10 - train_loss: 1.5502 - train_acc: 0.9135 - val_loss: 1.5617 - val_acc: 0.9010\n",
            "  Epoch 4/10 - train_loss: 1.5400 - train_acc: 0.9226 - val_loss: 1.5652 - val_acc: 0.8965\n",
            "  Epoch 5/10 - train_loss: 1.5283 - train_acc: 0.9336 - val_loss: 1.5384 - val_acc: 0.9234\n",
            "  Epoch 6/10 - train_loss: 1.5216 - train_acc: 0.9406 - val_loss: 1.5373 - val_acc: 0.9246\n",
            "  Epoch 7/10 - train_loss: 1.5205 - train_acc: 0.9410 - val_loss: 1.5453 - val_acc: 0.9161\n",
            "  Epoch 8/10 - train_loss: 1.5151 - train_acc: 0.9467 - val_loss: 1.5343 - val_acc: 0.9260\n",
            "  Epoch 9/10 - train_loss: 1.5128 - train_acc: 0.9490 - val_loss: 1.5352 - val_acc: 0.9264\n",
            "  Epoch 10/10 - train_loss: 1.5132 - train_acc: 0.9479 - val_loss: 1.5310 - val_acc: 0.9303\n",
            "Fold 2/3\n",
            "  Epoch 1/10 - train_loss: 1.8417 - train_acc: 0.7710 - val_loss: 1.6798 - val_acc: 0.8786\n",
            "  Epoch 2/10 - train_loss: 1.5888 - train_acc: 0.8797 - val_loss: 1.5698 - val_acc: 0.8922\n",
            "  Epoch 3/10 - train_loss: 1.5536 - train_acc: 0.9092 - val_loss: 1.5569 - val_acc: 0.9052\n",
            "  Epoch 4/10 - train_loss: 1.5403 - train_acc: 0.9216 - val_loss: 1.5441 - val_acc: 0.9179\n",
            "  Epoch 5/10 - train_loss: 1.5322 - train_acc: 0.9301 - val_loss: 1.5451 - val_acc: 0.9165\n",
            "  Epoch 6/10 - train_loss: 1.5262 - train_acc: 0.9359 - val_loss: 1.5444 - val_acc: 0.9175\n",
            "  Epoch 7/10 - train_loss: 1.5215 - train_acc: 0.9407 - val_loss: 1.5410 - val_acc: 0.9194\n",
            "  Epoch 8/10 - train_loss: 1.5189 - train_acc: 0.9424 - val_loss: 1.5446 - val_acc: 0.9163\n",
            "  Epoch 9/10 - train_loss: 1.5174 - train_acc: 0.9443 - val_loss: 1.5304 - val_acc: 0.9312\n",
            "  Epoch 10/10 - train_loss: 1.5135 - train_acc: 0.9479 - val_loss: 1.5350 - val_acc: 0.9257\n",
            "Fold 3/3\n",
            "  Epoch 1/10 - train_loss: 1.8441 - train_acc: 0.7682 - val_loss: 1.6835 - val_acc: 0.8777\n",
            "  Epoch 2/10 - train_loss: 1.5864 - train_acc: 0.8821 - val_loss: 1.5671 - val_acc: 0.8980\n",
            "  Epoch 3/10 - train_loss: 1.5541 - train_acc: 0.9093 - val_loss: 1.5595 - val_acc: 0.9022\n",
            "  Epoch 4/10 - train_loss: 1.5406 - train_acc: 0.9216 - val_loss: 1.5441 - val_acc: 0.9181\n",
            "  Epoch 5/10 - train_loss: 1.5345 - train_acc: 0.9279 - val_loss: 1.5402 - val_acc: 0.9217\n",
            "  Epoch 6/10 - train_loss: 1.5271 - train_acc: 0.9347 - val_loss: 1.5445 - val_acc: 0.9173\n",
            "  Epoch 7/10 - train_loss: 1.5234 - train_acc: 0.9385 - val_loss: 1.5322 - val_acc: 0.9292\n",
            "  Epoch 8/10 - train_loss: 1.5185 - train_acc: 0.9431 - val_loss: 1.5329 - val_acc: 0.9278\n",
            "  Epoch 9/10 - train_loss: 1.5146 - train_acc: 0.9467 - val_loss: 1.5385 - val_acc: 0.9224\n",
            "  Epoch 10/10 - train_loss: 1.5124 - train_acc: 0.9492 - val_loss: 1.5387 - val_acc: 0.9226\n",
            "<---------------- Testing adam with learning rate 0.001 and batch size 128 ---------------->\n",
            "Fold 1/3\n",
            "  Epoch 1/10 - train_loss: 1.8840 - train_acc: 0.7485 - val_loss: 1.7361 - val_acc: 0.8659\n",
            "  Epoch 2/10 - train_loss: 1.5920 - train_acc: 0.8801 - val_loss: 1.5852 - val_acc: 0.8810\n",
            "  Epoch 3/10 - train_loss: 1.5517 - train_acc: 0.9132 - val_loss: 1.5554 - val_acc: 0.9082\n",
            "  Epoch 4/10 - train_loss: 1.5382 - train_acc: 0.9260 - val_loss: 1.5502 - val_acc: 0.9124\n",
            "  Epoch 5/10 - train_loss: 1.5272 - train_acc: 0.9365 - val_loss: 1.5595 - val_acc: 0.9030\n",
            "  Epoch 6/10 - train_loss: 1.5208 - train_acc: 0.9416 - val_loss: 1.5332 - val_acc: 0.9285\n",
            "  Epoch 7/10 - train_loss: 1.5136 - train_acc: 0.9492 - val_loss: 1.5380 - val_acc: 0.9237\n",
            "  Epoch 8/10 - train_loss: 1.5117 - train_acc: 0.9505 - val_loss: 1.5355 - val_acc: 0.9251\n",
            "  Epoch 9/10 - train_loss: 1.5090 - train_acc: 0.9532 - val_loss: 1.5271 - val_acc: 0.9346\n",
            "  Epoch 10/10 - train_loss: 1.5048 - train_acc: 0.9574 - val_loss: 1.5306 - val_acc: 0.9310\n",
            "Fold 2/3\n",
            "  Epoch 1/10 - train_loss: 1.8825 - train_acc: 0.7488 - val_loss: 1.7429 - val_acc: 0.8613\n",
            "  Epoch 2/10 - train_loss: 1.5962 - train_acc: 0.8762 - val_loss: 1.5684 - val_acc: 0.8979\n",
            "  Epoch 3/10 - train_loss: 1.5554 - train_acc: 0.9092 - val_loss: 1.5582 - val_acc: 0.9053\n",
            "  Epoch 4/10 - train_loss: 1.5393 - train_acc: 0.9248 - val_loss: 1.5473 - val_acc: 0.9167\n",
            "  Epoch 5/10 - train_loss: 1.5293 - train_acc: 0.9338 - val_loss: 1.5435 - val_acc: 0.9189\n",
            "  Epoch 6/10 - train_loss: 1.5211 - train_acc: 0.9422 - val_loss: 1.5459 - val_acc: 0.9155\n",
            "  Epoch 7/10 - train_loss: 1.5166 - train_acc: 0.9460 - val_loss: 1.5349 - val_acc: 0.9276\n",
            "  Epoch 8/10 - train_loss: 1.5138 - train_acc: 0.9487 - val_loss: 1.5313 - val_acc: 0.9307\n",
            "  Epoch 9/10 - train_loss: 1.5109 - train_acc: 0.9515 - val_loss: 1.5384 - val_acc: 0.9242\n",
            "  Epoch 10/10 - train_loss: 1.5063 - train_acc: 0.9558 - val_loss: 1.5303 - val_acc: 0.9311\n",
            "Fold 3/3\n",
            "  Epoch 1/10 - train_loss: 1.8809 - train_acc: 0.7510 - val_loss: 1.7276 - val_acc: 0.8704\n",
            "  Epoch 2/10 - train_loss: 1.5955 - train_acc: 0.8775 - val_loss: 1.5852 - val_acc: 0.8818\n",
            "  Epoch 3/10 - train_loss: 1.5549 - train_acc: 0.9093 - val_loss: 1.5508 - val_acc: 0.9130\n",
            "  Epoch 4/10 - train_loss: 1.5387 - train_acc: 0.9253 - val_loss: 1.5444 - val_acc: 0.9203\n",
            "  Epoch 5/10 - train_loss: 1.5288 - train_acc: 0.9347 - val_loss: 1.5507 - val_acc: 0.9129\n",
            "  Epoch 6/10 - train_loss: 1.5244 - train_acc: 0.9383 - val_loss: 1.5360 - val_acc: 0.9265\n",
            "  Epoch 7/10 - train_loss: 1.5164 - train_acc: 0.9462 - val_loss: 1.5446 - val_acc: 0.9176\n",
            "  Epoch 8/10 - train_loss: 1.5124 - train_acc: 0.9503 - val_loss: 1.5263 - val_acc: 0.9354\n",
            "  Epoch 9/10 - train_loss: 1.5098 - train_acc: 0.9532 - val_loss: 1.5246 - val_acc: 0.9370\n",
            "  Epoch 10/10 - train_loss: 1.5066 - train_acc: 0.9558 - val_loss: 1.5278 - val_acc: 0.9341\n",
            "<---------------- Testing rmsprop with learning rate 0.1 and batch size 32 ---------------->\n",
            "Fold 1/3\n",
            "  Epoch 1/10 - train_loss: 1.8060 - train_acc: 0.6566 - val_loss: 1.6460 - val_acc: 0.8149\n",
            "  Epoch 2/10 - train_loss: 1.7027 - train_acc: 0.7579 - val_loss: 1.6687 - val_acc: 0.7920\n",
            "  Epoch 3/10 - train_loss: 1.7176 - train_acc: 0.7431 - val_loss: 1.6888 - val_acc: 0.7721\n",
            "  Epoch 4/10 - train_loss: 1.7367 - train_acc: 0.7242 - val_loss: 1.8086 - val_acc: 0.6524\n",
            "  Epoch 5/10 - train_loss: 1.7414 - train_acc: 0.7197 - val_loss: 1.7326 - val_acc: 0.7286\n",
            "  Epoch 6/10 - train_loss: 1.7697 - train_acc: 0.6913 - val_loss: 1.9835 - val_acc: 0.4776\n",
            "  Epoch 7/10 - train_loss: 1.7625 - train_acc: 0.6985 - val_loss: 1.7591 - val_acc: 0.7020\n",
            "  Epoch 8/10 - train_loss: 1.8082 - train_acc: 0.6530 - val_loss: 1.7786 - val_acc: 0.6825\n",
            "  Epoch 9/10 - train_loss: 1.8905 - train_acc: 0.5707 - val_loss: 1.7760 - val_acc: 0.6851\n",
            "  Epoch 10/10 - train_loss: 1.7720 - train_acc: 0.6891 - val_loss: 1.7755 - val_acc: 0.6857\n",
            "Fold 2/3\n",
            "  Epoch 1/10 - train_loss: 1.8463 - train_acc: 0.6160 - val_loss: 1.7214 - val_acc: 0.7392\n",
            "  Epoch 2/10 - train_loss: 1.7719 - train_acc: 0.6879 - val_loss: 1.8188 - val_acc: 0.6417\n",
            "  Epoch 3/10 - train_loss: 1.7921 - train_acc: 0.6687 - val_loss: 1.8745 - val_acc: 0.5861\n",
            "  Epoch 4/10 - train_loss: 1.8219 - train_acc: 0.6390 - val_loss: 1.8080 - val_acc: 0.6528\n",
            "  Epoch 5/10 - train_loss: 1.8458 - train_acc: 0.6152 - val_loss: 1.7643 - val_acc: 0.6968\n",
            "  Epoch 6/10 - train_loss: 1.8435 - train_acc: 0.6176 - val_loss: 1.8812 - val_acc: 0.5799\n",
            "  Epoch 7/10 - train_loss: 1.8677 - train_acc: 0.5934 - val_loss: 1.8897 - val_acc: 0.5715\n",
            "  Epoch 8/10 - train_loss: 1.8409 - train_acc: 0.6202 - val_loss: 1.8702 - val_acc: 0.5908\n",
            "  Epoch 9/10 - train_loss: 1.8881 - train_acc: 0.5730 - val_loss: 1.8762 - val_acc: 0.5849\n",
            "  Epoch 10/10 - train_loss: 1.9070 - train_acc: 0.5542 - val_loss: 1.8647 - val_acc: 0.5964\n",
            "Fold 3/3\n",
            "  Epoch 1/10 - train_loss: 1.8587 - train_acc: 0.6031 - val_loss: 1.6993 - val_acc: 0.7622\n",
            "  Epoch 2/10 - train_loss: 1.7735 - train_acc: 0.6870 - val_loss: 1.7538 - val_acc: 0.7068\n",
            "  Epoch 3/10 - train_loss: 1.8013 - train_acc: 0.6596 - val_loss: 1.7570 - val_acc: 0.7039\n",
            "  Epoch 4/10 - train_loss: 1.8396 - train_acc: 0.6214 - val_loss: 2.0026 - val_acc: 0.4586\n",
            "  Epoch 5/10 - train_loss: 1.8619 - train_acc: 0.5994 - val_loss: 1.8635 - val_acc: 0.5977\n",
            "  Epoch 6/10 - train_loss: 1.8895 - train_acc: 0.5716 - val_loss: 1.8919 - val_acc: 0.5692\n",
            "  Epoch 7/10 - train_loss: 1.9658 - train_acc: 0.4953 - val_loss: 1.8644 - val_acc: 0.5968\n",
            "  Epoch 8/10 - train_loss: 1.8846 - train_acc: 0.5765 - val_loss: 1.8798 - val_acc: 0.5813\n",
            "  Epoch 9/10 - train_loss: 1.8785 - train_acc: 0.5827 - val_loss: 1.8753 - val_acc: 0.5858\n",
            "  Epoch 10/10 - train_loss: 1.8967 - train_acc: 0.5645 - val_loss: 1.8801 - val_acc: 0.5810\n",
            "<---------------- Testing rmsprop with learning rate 0.1 and batch size 64 ---------------->\n",
            "Fold 1/3\n",
            "  Epoch 1/10 - train_loss: 1.7595 - train_acc: 0.7043 - val_loss: 1.6393 - val_acc: 0.8228\n",
            "  Epoch 2/10 - train_loss: 1.6866 - train_acc: 0.7737 - val_loss: 1.6701 - val_acc: 0.7902\n",
            "  Epoch 3/10 - train_loss: 1.6976 - train_acc: 0.7631 - val_loss: 1.7126 - val_acc: 0.7478\n",
            "  Epoch 4/10 - train_loss: 1.7125 - train_acc: 0.7483 - val_loss: 1.7150 - val_acc: 0.7458\n",
            "  Epoch 5/10 - train_loss: 1.7337 - train_acc: 0.7274 - val_loss: 1.7481 - val_acc: 0.7128\n",
            "  Epoch 6/10 - train_loss: 1.7426 - train_acc: 0.7184 - val_loss: 1.7288 - val_acc: 0.7323\n",
            "  Epoch 7/10 - train_loss: 1.7722 - train_acc: 0.6888 - val_loss: 1.7403 - val_acc: 0.7207\n",
            "  Epoch 8/10 - train_loss: 1.7721 - train_acc: 0.6889 - val_loss: 1.7291 - val_acc: 0.7320\n",
            "  Epoch 9/10 - train_loss: 1.7949 - train_acc: 0.6662 - val_loss: 1.7041 - val_acc: 0.7572\n",
            "  Epoch 10/10 - train_loss: 1.8147 - train_acc: 0.6464 - val_loss: 1.9248 - val_acc: 0.5364\n",
            "Fold 2/3\n",
            "  Epoch 1/10 - train_loss: 1.7411 - train_acc: 0.7235 - val_loss: 1.6310 - val_acc: 0.8299\n",
            "  Epoch 2/10 - train_loss: 1.6974 - train_acc: 0.7628 - val_loss: 1.6636 - val_acc: 0.7967\n",
            "  Epoch 3/10 - train_loss: 1.7160 - train_acc: 0.7448 - val_loss: 1.7193 - val_acc: 0.7417\n",
            "  Epoch 4/10 - train_loss: 1.7511 - train_acc: 0.7098 - val_loss: 1.7684 - val_acc: 0.6925\n",
            "  Epoch 5/10 - train_loss: 1.7834 - train_acc: 0.6776 - val_loss: 1.7719 - val_acc: 0.6895\n",
            "  Epoch 6/10 - train_loss: 1.8312 - train_acc: 0.6299 - val_loss: 1.8424 - val_acc: 0.6188\n",
            "  Epoch 7/10 - train_loss: 1.8443 - train_acc: 0.6168 - val_loss: 1.9120 - val_acc: 0.5491\n",
            "  Epoch 8/10 - train_loss: 1.9150 - train_acc: 0.5462 - val_loss: 1.7464 - val_acc: 0.7147\n",
            "  Epoch 9/10 - train_loss: 1.9282 - train_acc: 0.5330 - val_loss: 1.8423 - val_acc: 0.6188\n",
            "  Epoch 10/10 - train_loss: 1.9242 - train_acc: 0.5369 - val_loss: 2.2621 - val_acc: 0.1991\n",
            "Fold 3/3\n",
            "  Epoch 1/10 - train_loss: 1.7940 - train_acc: 0.6700 - val_loss: 1.6885 - val_acc: 0.7724\n",
            "  Epoch 2/10 - train_loss: 1.7512 - train_acc: 0.7087 - val_loss: 1.6774 - val_acc: 0.7823\n",
            "  Epoch 3/10 - train_loss: 1.7214 - train_acc: 0.7392 - val_loss: 1.7224 - val_acc: 0.7385\n",
            "  Epoch 4/10 - train_loss: 1.7345 - train_acc: 0.7265 - val_loss: 1.7219 - val_acc: 0.7389\n",
            "  Epoch 5/10 - train_loss: 1.7755 - train_acc: 0.6855 - val_loss: 1.7285 - val_acc: 0.7327\n",
            "  Epoch 6/10 - train_loss: 1.7838 - train_acc: 0.6773 - val_loss: 1.7242 - val_acc: 0.7368\n",
            "  Epoch 7/10 - train_loss: 1.8128 - train_acc: 0.6484 - val_loss: 1.8457 - val_acc: 0.6155\n",
            "  Epoch 8/10 - train_loss: 1.8138 - train_acc: 0.6473 - val_loss: 1.8414 - val_acc: 0.6196\n",
            "  Epoch 9/10 - train_loss: 1.7935 - train_acc: 0.6676 - val_loss: 1.8381 - val_acc: 0.6231\n",
            "  Epoch 10/10 - train_loss: 1.8303 - train_acc: 0.6309 - val_loss: 1.9180 - val_acc: 0.5432\n",
            "<---------------- Testing rmsprop with learning rate 0.1 and batch size 128 ---------------->\n",
            "Fold 1/3\n",
            "  Epoch 1/10 - train_loss: 1.7490 - train_acc: 0.7168 - val_loss: 1.6448 - val_acc: 0.8166\n",
            "  Epoch 2/10 - train_loss: 1.7040 - train_acc: 0.7559 - val_loss: 1.6708 - val_acc: 0.7890\n",
            "  Epoch 3/10 - train_loss: 1.6778 - train_acc: 0.7831 - val_loss: 1.6532 - val_acc: 0.8082\n",
            "  Epoch 4/10 - train_loss: 1.7019 - train_acc: 0.7590 - val_loss: 1.7195 - val_acc: 0.7410\n",
            "  Epoch 5/10 - train_loss: 1.6900 - train_acc: 0.7710 - val_loss: 1.8227 - val_acc: 0.6381\n",
            "  Epoch 6/10 - train_loss: 1.7203 - train_acc: 0.7405 - val_loss: 1.6935 - val_acc: 0.7675\n",
            "  Epoch 7/10 - train_loss: 1.7252 - train_acc: 0.7359 - val_loss: 1.8159 - val_acc: 0.6454\n",
            "  Epoch 8/10 - train_loss: 1.7279 - train_acc: 0.7329 - val_loss: 1.6758 - val_acc: 0.7849\n",
            "  Epoch 9/10 - train_loss: 1.7167 - train_acc: 0.7441 - val_loss: 1.7361 - val_acc: 0.7247\n",
            "  Epoch 10/10 - train_loss: 1.7482 - train_acc: 0.7129 - val_loss: 1.7041 - val_acc: 0.7572\n",
            "Fold 2/3\n",
            "  Epoch 1/10 - train_loss: 1.7943 - train_acc: 0.6689 - val_loss: 1.6350 - val_acc: 0.8288\n",
            "  Epoch 2/10 - train_loss: 1.7046 - train_acc: 0.7552 - val_loss: 1.6674 - val_acc: 0.7925\n",
            "  Epoch 3/10 - train_loss: 1.6585 - train_acc: 0.8021 - val_loss: 1.6705 - val_acc: 0.7903\n",
            "  Epoch 4/10 - train_loss: 1.6689 - train_acc: 0.7917 - val_loss: 1.6546 - val_acc: 0.8060\n",
            "  Epoch 5/10 - train_loss: 1.6873 - train_acc: 0.7736 - val_loss: 1.6903 - val_acc: 0.7711\n",
            "  Epoch 6/10 - train_loss: 1.6883 - train_acc: 0.7724 - val_loss: 1.6624 - val_acc: 0.7985\n",
            "  Epoch 7/10 - train_loss: 1.7035 - train_acc: 0.7575 - val_loss: 1.6529 - val_acc: 0.8085\n",
            "  Epoch 8/10 - train_loss: 1.7847 - train_acc: 0.6765 - val_loss: 1.8238 - val_acc: 0.6372\n",
            "  Epoch 9/10 - train_loss: 1.7659 - train_acc: 0.6949 - val_loss: 1.6967 - val_acc: 0.7641\n",
            "  Epoch 10/10 - train_loss: 1.7710 - train_acc: 0.6902 - val_loss: 1.7997 - val_acc: 0.6613\n",
            "Fold 3/3\n",
            "  Epoch 1/10 - train_loss: 1.7954 - train_acc: 0.6689 - val_loss: 1.7086 - val_acc: 0.7524\n",
            "  Epoch 2/10 - train_loss: 1.7463 - train_acc: 0.7134 - val_loss: 1.6596 - val_acc: 0.8012\n",
            "  Epoch 3/10 - train_loss: 1.6895 - train_acc: 0.7713 - val_loss: 1.6843 - val_acc: 0.7761\n",
            "  Epoch 4/10 - train_loss: 1.7103 - train_acc: 0.7502 - val_loss: 1.7143 - val_acc: 0.7463\n",
            "  Epoch 5/10 - train_loss: 1.7200 - train_acc: 0.7409 - val_loss: 1.7234 - val_acc: 0.7378\n",
            "  Epoch 6/10 - train_loss: 1.7205 - train_acc: 0.7405 - val_loss: 1.8197 - val_acc: 0.6416\n",
            "  Epoch 7/10 - train_loss: 1.7511 - train_acc: 0.7099 - val_loss: 1.7620 - val_acc: 0.6994\n",
            "  Epoch 8/10 - train_loss: 1.7585 - train_acc: 0.7025 - val_loss: 1.7584 - val_acc: 0.7029\n",
            "  Epoch 9/10 - train_loss: 1.7765 - train_acc: 0.6844 - val_loss: 1.7201 - val_acc: 0.7406\n",
            "  Epoch 10/10 - train_loss: 1.7515 - train_acc: 0.7096 - val_loss: 1.7142 - val_acc: 0.7463\n",
            "<---------------- Testing rmsprop with learning rate 0.01 and batch size 32 ---------------->\n",
            "Fold 1/3\n",
            "  Epoch 1/10 - train_loss: 1.7040 - train_acc: 0.7811 - val_loss: 1.5927 - val_acc: 0.8775\n",
            "  Epoch 2/10 - train_loss: 1.6131 - train_acc: 0.8484 - val_loss: 1.5902 - val_acc: 0.8702\n",
            "  Epoch 3/10 - train_loss: 1.6120 - train_acc: 0.8485 - val_loss: 1.5993 - val_acc: 0.8611\n",
            "  Epoch 4/10 - train_loss: 1.6144 - train_acc: 0.8461 - val_loss: 1.6090 - val_acc: 0.8521\n",
            "  Epoch 5/10 - train_loss: 1.6320 - train_acc: 0.8289 - val_loss: 1.6466 - val_acc: 0.8142\n",
            "  Epoch 6/10 - train_loss: 1.6378 - train_acc: 0.8233 - val_loss: 1.6390 - val_acc: 0.8220\n",
            "  Epoch 7/10 - train_loss: 1.6739 - train_acc: 0.7871 - val_loss: 1.6198 - val_acc: 0.8411\n",
            "  Epoch 8/10 - train_loss: 1.6751 - train_acc: 0.7859 - val_loss: 1.6788 - val_acc: 0.7823\n",
            "  Epoch 9/10 - train_loss: 1.6636 - train_acc: 0.7974 - val_loss: 1.6752 - val_acc: 0.7859\n",
            "  Epoch 10/10 - train_loss: 1.6924 - train_acc: 0.7687 - val_loss: 1.7368 - val_acc: 0.7242\n",
            "Fold 2/3\n",
            "  Epoch 1/10 - train_loss: 1.7071 - train_acc: 0.7795 - val_loss: 1.5851 - val_acc: 0.8849\n",
            "  Epoch 2/10 - train_loss: 1.6133 - train_acc: 0.8480 - val_loss: 1.6098 - val_acc: 0.8508\n",
            "  Epoch 3/10 - train_loss: 1.6091 - train_acc: 0.8519 - val_loss: 1.6757 - val_acc: 0.7844\n",
            "  Epoch 4/10 - train_loss: 1.6195 - train_acc: 0.8414 - val_loss: 1.6070 - val_acc: 0.8538\n",
            "  Epoch 5/10 - train_loss: 1.6366 - train_acc: 0.8243 - val_loss: 1.5995 - val_acc: 0.8612\n",
            "  Epoch 6/10 - train_loss: 1.6303 - train_acc: 0.8306 - val_loss: 1.6547 - val_acc: 0.8062\n",
            "  Epoch 7/10 - train_loss: 1.6381 - train_acc: 0.8226 - val_loss: 1.6588 - val_acc: 0.8022\n",
            "  Epoch 8/10 - train_loss: 1.6369 - train_acc: 0.8240 - val_loss: 1.7053 - val_acc: 0.7559\n",
            "  Epoch 9/10 - train_loss: 1.6635 - train_acc: 0.7975 - val_loss: 1.7468 - val_acc: 0.7142\n",
            "  Epoch 10/10 - train_loss: 1.6807 - train_acc: 0.7803 - val_loss: 1.6970 - val_acc: 0.7641\n",
            "Fold 3/3\n",
            "  Epoch 1/10 - train_loss: 1.6976 - train_acc: 0.7895 - val_loss: 1.5759 - val_acc: 0.8925\n",
            "  Epoch 2/10 - train_loss: 1.6156 - train_acc: 0.8450 - val_loss: 1.6898 - val_acc: 0.7704\n",
            "  Epoch 3/10 - train_loss: 1.6100 - train_acc: 0.8501 - val_loss: 1.6224 - val_acc: 0.8383\n",
            "  Epoch 4/10 - train_loss: 1.6296 - train_acc: 0.8310 - val_loss: 1.7130 - val_acc: 0.7481\n",
            "  Epoch 5/10 - train_loss: 1.6375 - train_acc: 0.8234 - val_loss: 1.7411 - val_acc: 0.7198\n",
            "  Epoch 6/10 - train_loss: 1.6500 - train_acc: 0.8107 - val_loss: 1.6734 - val_acc: 0.7873\n",
            "  Epoch 7/10 - train_loss: 1.6461 - train_acc: 0.8148 - val_loss: 1.7040 - val_acc: 0.7571\n",
            "  Epoch 8/10 - train_loss: 1.6416 - train_acc: 0.8194 - val_loss: 1.6196 - val_acc: 0.8415\n",
            "  Epoch 9/10 - train_loss: 1.6593 - train_acc: 0.8019 - val_loss: 1.6888 - val_acc: 0.7723\n",
            "  Epoch 10/10 - train_loss: 1.6717 - train_acc: 0.7893 - val_loss: 1.6272 - val_acc: 0.8339\n",
            "<---------------- Testing rmsprop with learning rate 0.01 and batch size 64 ---------------->\n",
            "Fold 1/3\n",
            "  Epoch 1/10 - train_loss: 1.6749 - train_acc: 0.8168 - val_loss: 1.5734 - val_acc: 0.8963\n",
            "  Epoch 2/10 - train_loss: 1.6061 - train_acc: 0.8549 - val_loss: 1.6009 - val_acc: 0.8598\n",
            "  Epoch 3/10 - train_loss: 1.5884 - train_acc: 0.8720 - val_loss: 1.6109 - val_acc: 0.8496\n",
            "  Epoch 4/10 - train_loss: 1.5939 - train_acc: 0.8667 - val_loss: 1.5904 - val_acc: 0.8702\n",
            "  Epoch 5/10 - train_loss: 1.5938 - train_acc: 0.8670 - val_loss: 1.6178 - val_acc: 0.8430\n",
            "  Epoch 6/10 - train_loss: 1.5976 - train_acc: 0.8632 - val_loss: 1.6043 - val_acc: 0.8565\n",
            "  Epoch 7/10 - train_loss: 1.5977 - train_acc: 0.8633 - val_loss: 1.5987 - val_acc: 0.8625\n",
            "  Epoch 8/10 - train_loss: 1.6048 - train_acc: 0.8561 - val_loss: 1.6289 - val_acc: 0.8320\n",
            "  Epoch 9/10 - train_loss: 1.6159 - train_acc: 0.8452 - val_loss: 1.6295 - val_acc: 0.8316\n",
            "  Epoch 10/10 - train_loss: 1.6270 - train_acc: 0.8340 - val_loss: 1.6528 - val_acc: 0.8083\n",
            "Fold 2/3\n",
            "  Epoch 1/10 - train_loss: 1.6757 - train_acc: 0.8147 - val_loss: 1.5700 - val_acc: 0.8993\n",
            "  Epoch 2/10 - train_loss: 1.6031 - train_acc: 0.8585 - val_loss: 1.5844 - val_acc: 0.8767\n",
            "  Epoch 3/10 - train_loss: 1.5921 - train_acc: 0.8686 - val_loss: 1.6357 - val_acc: 0.8244\n",
            "  Epoch 4/10 - train_loss: 1.5952 - train_acc: 0.8653 - val_loss: 1.6062 - val_acc: 0.8539\n",
            "  Epoch 5/10 - train_loss: 1.5904 - train_acc: 0.8702 - val_loss: 1.6256 - val_acc: 0.8351\n",
            "  Epoch 6/10 - train_loss: 1.6012 - train_acc: 0.8593 - val_loss: 1.5985 - val_acc: 0.8623\n",
            "  Epoch 7/10 - train_loss: 1.6123 - train_acc: 0.8486 - val_loss: 1.5926 - val_acc: 0.8686\n",
            "  Epoch 8/10 - train_loss: 1.6218 - train_acc: 0.8391 - val_loss: 1.5974 - val_acc: 0.8637\n",
            "  Epoch 9/10 - train_loss: 1.6350 - train_acc: 0.8260 - val_loss: 1.6525 - val_acc: 0.8084\n",
            "  Epoch 10/10 - train_loss: 1.6304 - train_acc: 0.8306 - val_loss: 1.6246 - val_acc: 0.8362\n",
            "Fold 3/3\n",
            "  Epoch 1/10 - train_loss: 1.6748 - train_acc: 0.8158 - val_loss: 1.5662 - val_acc: 0.9028\n",
            "  Epoch 2/10 - train_loss: 1.6045 - train_acc: 0.8568 - val_loss: 1.6067 - val_acc: 0.8546\n",
            "  Epoch 3/10 - train_loss: 1.5941 - train_acc: 0.8666 - val_loss: 1.5979 - val_acc: 0.8629\n",
            "  Epoch 4/10 - train_loss: 1.5946 - train_acc: 0.8657 - val_loss: 1.5766 - val_acc: 0.8843\n",
            "  Epoch 5/10 - train_loss: 1.5989 - train_acc: 0.8617 - val_loss: 1.6352 - val_acc: 0.8254\n",
            "  Epoch 6/10 - train_loss: 1.5974 - train_acc: 0.8631 - val_loss: 1.5917 - val_acc: 0.8693\n",
            "  Epoch 7/10 - train_loss: 1.6057 - train_acc: 0.8552 - val_loss: 1.6363 - val_acc: 0.8244\n",
            "  Epoch 8/10 - train_loss: 1.6056 - train_acc: 0.8552 - val_loss: 1.6419 - val_acc: 0.8192\n",
            "  Epoch 9/10 - train_loss: 1.6176 - train_acc: 0.8434 - val_loss: 1.6099 - val_acc: 0.8511\n",
            "  Epoch 10/10 - train_loss: 1.6200 - train_acc: 0.8409 - val_loss: 1.6298 - val_acc: 0.8310\n",
            "<---------------- Testing rmsprop with learning rate 0.01 and batch size 128 ---------------->\n",
            "Fold 1/3\n",
            "  Epoch 1/10 - train_loss: 1.6734 - train_acc: 0.8252 - val_loss: 1.5760 - val_acc: 0.8999\n",
            "  Epoch 2/10 - train_loss: 1.6181 - train_acc: 0.8435 - val_loss: 1.6005 - val_acc: 0.8606\n",
            "  Epoch 3/10 - train_loss: 1.5734 - train_acc: 0.8875 - val_loss: 1.6156 - val_acc: 0.8449\n",
            "  Epoch 4/10 - train_loss: 1.5715 - train_acc: 0.8892 - val_loss: 1.5592 - val_acc: 0.9012\n",
            "  Epoch 5/10 - train_loss: 1.5634 - train_acc: 0.8974 - val_loss: 1.5824 - val_acc: 0.8786\n",
            "  Epoch 6/10 - train_loss: 1.5660 - train_acc: 0.8944 - val_loss: 1.5833 - val_acc: 0.8772\n",
            "  Epoch 7/10 - train_loss: 1.5667 - train_acc: 0.8942 - val_loss: 1.5679 - val_acc: 0.8931\n",
            "  Epoch 8/10 - train_loss: 1.5660 - train_acc: 0.8949 - val_loss: 1.5892 - val_acc: 0.8720\n",
            "  Epoch 9/10 - train_loss: 1.5639 - train_acc: 0.8969 - val_loss: 1.6197 - val_acc: 0.8415\n",
            "  Epoch 10/10 - train_loss: 1.5754 - train_acc: 0.8853 - val_loss: 1.5916 - val_acc: 0.8698\n",
            "Fold 2/3\n",
            "  Epoch 1/10 - train_loss: 1.6715 - train_acc: 0.8266 - val_loss: 1.5777 - val_acc: 0.8959\n",
            "  Epoch 2/10 - train_loss: 1.6191 - train_acc: 0.8422 - val_loss: 1.5800 - val_acc: 0.8812\n",
            "  Epoch 3/10 - train_loss: 1.5732 - train_acc: 0.8877 - val_loss: 1.5762 - val_acc: 0.8844\n",
            "  Epoch 4/10 - train_loss: 1.5672 - train_acc: 0.8932 - val_loss: 1.5808 - val_acc: 0.8797\n",
            "  Epoch 5/10 - train_loss: 1.5670 - train_acc: 0.8940 - val_loss: 1.5544 - val_acc: 0.9062\n",
            "  Epoch 6/10 - train_loss: 1.5679 - train_acc: 0.8927 - val_loss: 1.5811 - val_acc: 0.8800\n",
            "  Epoch 7/10 - train_loss: 1.5736 - train_acc: 0.8873 - val_loss: 1.6144 - val_acc: 0.8464\n",
            "  Epoch 8/10 - train_loss: 1.5723 - train_acc: 0.8886 - val_loss: 1.6451 - val_acc: 0.8155\n",
            "  Epoch 9/10 - train_loss: 1.5748 - train_acc: 0.8858 - val_loss: 1.5999 - val_acc: 0.8611\n",
            "  Epoch 10/10 - train_loss: 1.5764 - train_acc: 0.8848 - val_loss: 1.6746 - val_acc: 0.7857\n",
            "Fold 3/3\n",
            "  Epoch 1/10 - train_loss: 1.6707 - train_acc: 0.8311 - val_loss: 1.5728 - val_acc: 0.9040\n",
            "  Epoch 2/10 - train_loss: 1.6144 - train_acc: 0.8468 - val_loss: 1.6207 - val_acc: 0.8407\n",
            "  Epoch 3/10 - train_loss: 1.5724 - train_acc: 0.8885 - val_loss: 1.5945 - val_acc: 0.8660\n",
            "  Epoch 4/10 - train_loss: 1.5678 - train_acc: 0.8929 - val_loss: 1.5678 - val_acc: 0.8925\n",
            "  Epoch 5/10 - train_loss: 1.5679 - train_acc: 0.8928 - val_loss: 1.6238 - val_acc: 0.8368\n",
            "  Epoch 6/10 - train_loss: 1.5674 - train_acc: 0.8934 - val_loss: 1.5881 - val_acc: 0.8718\n",
            "  Epoch 7/10 - train_loss: 1.5657 - train_acc: 0.8950 - val_loss: 1.6279 - val_acc: 0.8328\n",
            "  Epoch 8/10 - train_loss: 1.5693 - train_acc: 0.8917 - val_loss: 1.5610 - val_acc: 0.9001\n",
            "  Epoch 9/10 - train_loss: 1.5734 - train_acc: 0.8873 - val_loss: 1.5927 - val_acc: 0.8679\n",
            "  Epoch 10/10 - train_loss: 1.5822 - train_acc: 0.8788 - val_loss: 1.5872 - val_acc: 0.8737\n",
            "<---------------- Testing rmsprop with learning rate 0.001 and batch size 32 ---------------->\n",
            "Fold 1/3\n",
            "  Epoch 1/10 - train_loss: 1.7961 - train_acc: 0.7772 - val_loss: 1.6378 - val_acc: 0.8805\n",
            "  Epoch 2/10 - train_loss: 1.5765 - train_acc: 0.8905 - val_loss: 1.5660 - val_acc: 0.8974\n",
            "  Epoch 3/10 - train_loss: 1.5506 - train_acc: 0.9121 - val_loss: 1.5580 - val_acc: 0.9030\n",
            "  Epoch 4/10 - train_loss: 1.5420 - train_acc: 0.9197 - val_loss: 1.5520 - val_acc: 0.9094\n",
            "  Epoch 5/10 - train_loss: 1.5332 - train_acc: 0.9288 - val_loss: 1.5441 - val_acc: 0.9170\n",
            "  Epoch 6/10 - train_loss: 1.5307 - train_acc: 0.9304 - val_loss: 1.5427 - val_acc: 0.9185\n",
            "  Epoch 7/10 - train_loss: 1.5286 - train_acc: 0.9324 - val_loss: 1.5387 - val_acc: 0.9221\n",
            "  Epoch 8/10 - train_loss: 1.5264 - train_acc: 0.9344 - val_loss: 1.5451 - val_acc: 0.9156\n",
            "  Epoch 9/10 - train_loss: 1.5229 - train_acc: 0.9382 - val_loss: 1.5429 - val_acc: 0.9183\n",
            "  Epoch 10/10 - train_loss: 1.5225 - train_acc: 0.9382 - val_loss: 1.5409 - val_acc: 0.9199\n",
            "Fold 2/3\n",
            "  Epoch 1/10 - train_loss: 1.7923 - train_acc: 0.7818 - val_loss: 1.6319 - val_acc: 0.8850\n",
            "  Epoch 2/10 - train_loss: 1.5767 - train_acc: 0.8900 - val_loss: 1.5584 - val_acc: 0.9048\n",
            "  Epoch 3/10 - train_loss: 1.5537 - train_acc: 0.9085 - val_loss: 1.5577 - val_acc: 0.9038\n",
            "  Epoch 4/10 - train_loss: 1.5414 - train_acc: 0.9203 - val_loss: 1.5454 - val_acc: 0.9158\n",
            "  Epoch 5/10 - train_loss: 1.5346 - train_acc: 0.9268 - val_loss: 1.5419 - val_acc: 0.9188\n",
            "  Epoch 6/10 - train_loss: 1.5316 - train_acc: 0.9293 - val_loss: 1.5402 - val_acc: 0.9204\n",
            "  Epoch 7/10 - train_loss: 1.5271 - train_acc: 0.9343 - val_loss: 1.5431 - val_acc: 0.9182\n",
            "  Epoch 8/10 - train_loss: 1.5250 - train_acc: 0.9363 - val_loss: 1.5422 - val_acc: 0.9188\n",
            "  Epoch 9/10 - train_loss: 1.5246 - train_acc: 0.9363 - val_loss: 1.5354 - val_acc: 0.9255\n",
            "  Epoch 10/10 - train_loss: 1.5216 - train_acc: 0.9393 - val_loss: 1.5561 - val_acc: 0.9048\n",
            "Fold 3/3\n",
            "  Epoch 1/10 - train_loss: 1.7938 - train_acc: 0.7812 - val_loss: 1.6301 - val_acc: 0.8865\n",
            "  Epoch 2/10 - train_loss: 1.5789 - train_acc: 0.8875 - val_loss: 1.5620 - val_acc: 0.9010\n",
            "  Epoch 3/10 - train_loss: 1.5537 - train_acc: 0.9083 - val_loss: 1.5589 - val_acc: 0.9029\n",
            "  Epoch 4/10 - train_loss: 1.5429 - train_acc: 0.9190 - val_loss: 1.5504 - val_acc: 0.9112\n",
            "  Epoch 5/10 - train_loss: 1.5370 - train_acc: 0.9240 - val_loss: 1.5391 - val_acc: 0.9221\n",
            "  Epoch 6/10 - train_loss: 1.5327 - train_acc: 0.9286 - val_loss: 1.5429 - val_acc: 0.9185\n",
            "  Epoch 7/10 - train_loss: 1.5300 - train_acc: 0.9309 - val_loss: 1.5325 - val_acc: 0.9289\n",
            "  Epoch 8/10 - train_loss: 1.5263 - train_acc: 0.9349 - val_loss: 1.5406 - val_acc: 0.9206\n",
            "  Epoch 9/10 - train_loss: 1.5252 - train_acc: 0.9357 - val_loss: 1.5378 - val_acc: 0.9231\n",
            "  Epoch 10/10 - train_loss: 1.5231 - train_acc: 0.9380 - val_loss: 1.5416 - val_acc: 0.9194\n",
            "<---------------- Testing rmsprop with learning rate 0.001 and batch size 64 ---------------->\n",
            "Fold 1/3\n",
            "  Epoch 1/10 - train_loss: 1.7971 - train_acc: 0.8038 - val_loss: 1.6562 - val_acc: 0.8863\n",
            "  Epoch 2/10 - train_loss: 1.5775 - train_acc: 0.8919 - val_loss: 1.5562 - val_acc: 0.9083\n",
            "  Epoch 3/10 - train_loss: 1.5457 - train_acc: 0.9183 - val_loss: 1.5492 - val_acc: 0.9141\n",
            "  Epoch 4/10 - train_loss: 1.5336 - train_acc: 0.9289 - val_loss: 1.5459 - val_acc: 0.9159\n",
            "  Epoch 5/10 - train_loss: 1.5265 - train_acc: 0.9354 - val_loss: 1.5546 - val_acc: 0.9072\n",
            "  Epoch 6/10 - train_loss: 1.5206 - train_acc: 0.9417 - val_loss: 1.5285 - val_acc: 0.9325\n",
            "  Epoch 7/10 - train_loss: 1.5179 - train_acc: 0.9438 - val_loss: 1.5330 - val_acc: 0.9279\n",
            "  Epoch 8/10 - train_loss: 1.5145 - train_acc: 0.9471 - val_loss: 1.5342 - val_acc: 0.9268\n",
            "  Epoch 9/10 - train_loss: 1.5134 - train_acc: 0.9481 - val_loss: 1.5309 - val_acc: 0.9304\n",
            "  Epoch 10/10 - train_loss: 1.5106 - train_acc: 0.9508 - val_loss: 1.5276 - val_acc: 0.9334\n",
            "Fold 2/3\n",
            "  Epoch 1/10 - train_loss: 1.7949 - train_acc: 0.8052 - val_loss: 1.6624 - val_acc: 0.8882\n",
            "  Epoch 2/10 - train_loss: 1.5801 - train_acc: 0.8891 - val_loss: 1.5600 - val_acc: 0.9056\n",
            "  Epoch 3/10 - train_loss: 1.5498 - train_acc: 0.9136 - val_loss: 1.5501 - val_acc: 0.9126\n",
            "  Epoch 4/10 - train_loss: 1.5361 - train_acc: 0.9270 - val_loss: 1.5361 - val_acc: 0.9264\n",
            "  Epoch 5/10 - train_loss: 1.5298 - train_acc: 0.9318 - val_loss: 1.5448 - val_acc: 0.9166\n",
            "  Epoch 6/10 - train_loss: 1.5241 - train_acc: 0.9379 - val_loss: 1.5389 - val_acc: 0.9227\n",
            "  Epoch 7/10 - train_loss: 1.5194 - train_acc: 0.9419 - val_loss: 1.5445 - val_acc: 0.9164\n",
            "  Epoch 8/10 - train_loss: 1.5160 - train_acc: 0.9454 - val_loss: 1.5333 - val_acc: 0.9279\n",
            "  Epoch 9/10 - train_loss: 1.5129 - train_acc: 0.9485 - val_loss: 1.5399 - val_acc: 0.9214\n",
            "  Epoch 10/10 - train_loss: 1.5129 - train_acc: 0.9487 - val_loss: 1.5341 - val_acc: 0.9272\n",
            "Fold 3/3\n",
            "  Epoch 1/10 - train_loss: 1.7990 - train_acc: 0.7992 - val_loss: 1.6568 - val_acc: 0.8913\n",
            "  Epoch 2/10 - train_loss: 1.5782 - train_acc: 0.8909 - val_loss: 1.5594 - val_acc: 0.9052\n",
            "  Epoch 3/10 - train_loss: 1.5483 - train_acc: 0.9156 - val_loss: 1.5627 - val_acc: 0.9004\n",
            "  Epoch 4/10 - train_loss: 1.5356 - train_acc: 0.9269 - val_loss: 1.5400 - val_acc: 0.9233\n",
            "  Epoch 5/10 - train_loss: 1.5290 - train_acc: 0.9329 - val_loss: 1.5345 - val_acc: 0.9279\n",
            "  Epoch 6/10 - train_loss: 1.5226 - train_acc: 0.9396 - val_loss: 1.5412 - val_acc: 0.9198\n",
            "  Epoch 7/10 - train_loss: 1.5180 - train_acc: 0.9435 - val_loss: 1.5450 - val_acc: 0.9161\n",
            "  Epoch 8/10 - train_loss: 1.5151 - train_acc: 0.9466 - val_loss: 1.5421 - val_acc: 0.9183\n",
            "  Epoch 9/10 - train_loss: 1.5135 - train_acc: 0.9475 - val_loss: 1.5303 - val_acc: 0.9313\n",
            "  Epoch 10/10 - train_loss: 1.5113 - train_acc: 0.9499 - val_loss: 1.5326 - val_acc: 0.9279\n",
            "<---------------- Testing rmsprop with learning rate 0.001 and batch size 128 ---------------->\n",
            "Fold 1/3\n",
            "  Epoch 1/10 - train_loss: 1.8122 - train_acc: 0.8067 - val_loss: 1.7018 - val_acc: 0.8828\n",
            "  Epoch 2/10 - train_loss: 1.5846 - train_acc: 0.8882 - val_loss: 1.5625 - val_acc: 0.9030\n",
            "  Epoch 3/10 - train_loss: 1.5470 - train_acc: 0.9182 - val_loss: 1.5546 - val_acc: 0.9099\n",
            "  Epoch 4/10 - train_loss: 1.5331 - train_acc: 0.9308 - val_loss: 1.5635 - val_acc: 0.8990\n",
            "  Epoch 5/10 - train_loss: 1.5239 - train_acc: 0.9392 - val_loss: 1.5391 - val_acc: 0.9232\n",
            "  Epoch 6/10 - train_loss: 1.5181 - train_acc: 0.9448 - val_loss: 1.5522 - val_acc: 0.9099\n",
            "  Epoch 7/10 - train_loss: 1.5154 - train_acc: 0.9469 - val_loss: 1.5397 - val_acc: 0.9225\n",
            "  Epoch 8/10 - train_loss: 1.5106 - train_acc: 0.9520 - val_loss: 1.5418 - val_acc: 0.9205\n",
            "  Epoch 9/10 - train_loss: 1.5072 - train_acc: 0.9549 - val_loss: 1.5417 - val_acc: 0.9198\n",
            "  Epoch 10/10 - train_loss: 1.5056 - train_acc: 0.9564 - val_loss: 1.5303 - val_acc: 0.9313\n",
            "Fold 2/3\n",
            "  Epoch 1/10 - train_loss: 1.8143 - train_acc: 0.8041 - val_loss: 1.7003 - val_acc: 0.8766\n",
            "  Epoch 2/10 - train_loss: 1.5877 - train_acc: 0.8846 - val_loss: 1.5866 - val_acc: 0.8779\n",
            "  Epoch 3/10 - train_loss: 1.5511 - train_acc: 0.9142 - val_loss: 1.5576 - val_acc: 0.9056\n",
            "  Epoch 4/10 - train_loss: 1.5382 - train_acc: 0.9253 - val_loss: 1.5754 - val_acc: 0.8873\n",
            "  Epoch 5/10 - train_loss: 1.5283 - train_acc: 0.9350 - val_loss: 1.5429 - val_acc: 0.9199\n",
            "  Epoch 6/10 - train_loss: 1.5228 - train_acc: 0.9397 - val_loss: 1.5430 - val_acc: 0.9196\n",
            "  Epoch 7/10 - train_loss: 1.5167 - train_acc: 0.9458 - val_loss: 1.5377 - val_acc: 0.9251\n",
            "  Epoch 8/10 - train_loss: 1.5121 - train_acc: 0.9503 - val_loss: 1.5284 - val_acc: 0.9334\n",
            "  Epoch 9/10 - train_loss: 1.5100 - train_acc: 0.9519 - val_loss: 1.5423 - val_acc: 0.9195\n",
            "  Epoch 10/10 - train_loss: 1.5064 - train_acc: 0.9557 - val_loss: 1.5313 - val_acc: 0.9300\n",
            "Fold 3/3\n",
            "  Epoch 1/10 - train_loss: 1.8160 - train_acc: 0.8098 - val_loss: 1.7012 - val_acc: 0.8838\n",
            "  Epoch 2/10 - train_loss: 1.5858 - train_acc: 0.8862 - val_loss: 1.5960 - val_acc: 0.8706\n",
            "  Epoch 3/10 - train_loss: 1.5499 - train_acc: 0.9150 - val_loss: 1.5606 - val_acc: 0.9035\n",
            "  Epoch 4/10 - train_loss: 1.5346 - train_acc: 0.9300 - val_loss: 1.5909 - val_acc: 0.8734\n",
            "  Epoch 5/10 - train_loss: 1.5260 - train_acc: 0.9375 - val_loss: 1.5403 - val_acc: 0.9221\n",
            "  Epoch 6/10 - train_loss: 1.5194 - train_acc: 0.9437 - val_loss: 1.5454 - val_acc: 0.9167\n",
            "  Epoch 7/10 - train_loss: 1.5142 - train_acc: 0.9486 - val_loss: 1.5317 - val_acc: 0.9304\n",
            "  Epoch 8/10 - train_loss: 1.5110 - train_acc: 0.9512 - val_loss: 1.5490 - val_acc: 0.9129\n",
            "  Epoch 9/10 - train_loss: 1.5073 - train_acc: 0.9550 - val_loss: 1.5228 - val_acc: 0.9382\n",
            "  Epoch 10/10 - train_loss: 1.5059 - train_acc: 0.9559 - val_loss: 1.5191 - val_acc: 0.9420\n",
            "<---------------- Testing adamw with learning rate 0.1 and batch size 32 ---------------->\n",
            "Fold 1/3\n",
            "  Epoch 1/10 - train_loss: 1.7581 - train_acc: 0.7117 - val_loss: 1.6760 - val_acc: 0.7870\n",
            "  Epoch 2/10 - train_loss: 1.8413 - train_acc: 0.6192 - val_loss: 1.6859 - val_acc: 0.7754\n",
            "  Epoch 3/10 - train_loss: 1.6926 - train_acc: 0.7681 - val_loss: 1.7322 - val_acc: 0.7281\n",
            "  Epoch 4/10 - train_loss: 1.6896 - train_acc: 0.7727 - val_loss: 1.7043 - val_acc: 0.7574\n",
            "  Epoch 5/10 - train_loss: 1.6947 - train_acc: 0.7666 - val_loss: 1.6858 - val_acc: 0.7750\n",
            "  Epoch 6/10 - train_loss: 1.7032 - train_acc: 0.7582 - val_loss: 1.7207 - val_acc: 0.7401\n",
            "  Epoch 7/10 - train_loss: 1.7011 - train_acc: 0.7602 - val_loss: 1.6810 - val_acc: 0.7797\n",
            "  Epoch 8/10 - train_loss: 1.7077 - train_acc: 0.7534 - val_loss: 1.7266 - val_acc: 0.7346\n",
            "  Epoch 9/10 - train_loss: 1.7014 - train_acc: 0.7598 - val_loss: 1.6968 - val_acc: 0.7655\n",
            "  Epoch 10/10 - train_loss: 1.7030 - train_acc: 0.7587 - val_loss: 1.7114 - val_acc: 0.7491\n",
            "Fold 2/3\n",
            "  Epoch 1/10 - train_loss: 1.8133 - train_acc: 0.6540 - val_loss: 1.7309 - val_acc: 0.7333\n",
            "  Epoch 2/10 - train_loss: 1.7632 - train_acc: 0.6976 - val_loss: 1.7348 - val_acc: 0.7268\n",
            "  Epoch 3/10 - train_loss: 1.7412 - train_acc: 0.7195 - val_loss: 1.7140 - val_acc: 0.7472\n",
            "  Epoch 4/10 - train_loss: 1.7048 - train_acc: 0.7564 - val_loss: 1.7176 - val_acc: 0.7436\n",
            "  Epoch 5/10 - train_loss: 1.7088 - train_acc: 0.7523 - val_loss: 1.6979 - val_acc: 0.7638\n",
            "  Epoch 6/10 - train_loss: 1.6955 - train_acc: 0.7658 - val_loss: 1.6847 - val_acc: 0.7756\n",
            "  Epoch 7/10 - train_loss: 1.6991 - train_acc: 0.7624 - val_loss: 1.7006 - val_acc: 0.7608\n",
            "  Epoch 8/10 - train_loss: 1.7040 - train_acc: 0.7567 - val_loss: 1.7162 - val_acc: 0.7446\n",
            "  Epoch 9/10 - train_loss: 1.6981 - train_acc: 0.7632 - val_loss: 1.7174 - val_acc: 0.7426\n",
            "  Epoch 10/10 - train_loss: 1.7019 - train_acc: 0.7589 - val_loss: 1.6787 - val_acc: 0.7837\n",
            "Fold 3/3\n",
            "  Epoch 1/10 - train_loss: 1.7655 - train_acc: 0.7029 - val_loss: 1.6476 - val_acc: 0.8168\n",
            "  Epoch 2/10 - train_loss: 1.7256 - train_acc: 0.7355 - val_loss: 1.6479 - val_acc: 0.8144\n",
            "  Epoch 3/10 - train_loss: 1.6749 - train_acc: 0.7865 - val_loss: 1.6646 - val_acc: 0.7986\n",
            "  Epoch 4/10 - train_loss: 1.6628 - train_acc: 0.7987 - val_loss: 1.6452 - val_acc: 0.8169\n",
            "  Epoch 5/10 - train_loss: 1.6798 - train_acc: 0.7812 - val_loss: 1.6846 - val_acc: 0.7763\n",
            "  Epoch 6/10 - train_loss: 1.6734 - train_acc: 0.7878 - val_loss: 1.7207 - val_acc: 0.7401\n",
            "  Epoch 7/10 - train_loss: 1.6740 - train_acc: 0.7874 - val_loss: 1.6597 - val_acc: 0.8020\n",
            "  Epoch 8/10 - train_loss: 1.6841 - train_acc: 0.7774 - val_loss: 1.7096 - val_acc: 0.7515\n",
            "  Epoch 9/10 - train_loss: 1.6778 - train_acc: 0.7830 - val_loss: 1.6907 - val_acc: 0.7708\n",
            "  Epoch 10/10 - train_loss: 1.6786 - train_acc: 0.7829 - val_loss: 1.6342 - val_acc: 0.8283\n",
            "<---------------- Testing adamw with learning rate 0.1 and batch size 64 ---------------->\n",
            "Fold 1/3\n",
            "  Epoch 1/10 - train_loss: 1.7223 - train_acc: 0.7491 - val_loss: 1.6179 - val_acc: 0.8461\n",
            "  Epoch 2/10 - train_loss: 1.7601 - train_acc: 0.6995 - val_loss: 1.6737 - val_acc: 0.7886\n",
            "  Epoch 3/10 - train_loss: 1.6459 - train_acc: 0.8160 - val_loss: 1.6216 - val_acc: 0.8395\n",
            "  Epoch 4/10 - train_loss: 1.6332 - train_acc: 0.8279 - val_loss: 1.6219 - val_acc: 0.8403\n",
            "  Epoch 5/10 - train_loss: 1.6308 - train_acc: 0.8306 - val_loss: 1.6597 - val_acc: 0.8016\n",
            "  Epoch 6/10 - train_loss: 1.6351 - train_acc: 0.8253 - val_loss: 1.6350 - val_acc: 0.8263\n",
            "  Epoch 7/10 - train_loss: 1.6281 - train_acc: 0.8329 - val_loss: 1.6420 - val_acc: 0.8190\n",
            "  Epoch 8/10 - train_loss: 1.6260 - train_acc: 0.8349 - val_loss: 1.6477 - val_acc: 0.8121\n",
            "  Epoch 9/10 - train_loss: 1.6356 - train_acc: 0.8259 - val_loss: 1.6509 - val_acc: 0.8098\n",
            "  Epoch 10/10 - train_loss: 1.6378 - train_acc: 0.8230 - val_loss: 1.6679 - val_acc: 0.7923\n",
            "Fold 2/3\n",
            "  Epoch 1/10 - train_loss: 1.7749 - train_acc: 0.6933 - val_loss: 1.6409 - val_acc: 0.8239\n",
            "  Epoch 2/10 - train_loss: 1.8921 - train_acc: 0.5677 - val_loss: 1.7850 - val_acc: 0.6752\n",
            "  Epoch 3/10 - train_loss: 1.7047 - train_acc: 0.7559 - val_loss: 1.6552 - val_acc: 0.8061\n",
            "  Epoch 4/10 - train_loss: 1.6612 - train_acc: 0.8002 - val_loss: 1.6575 - val_acc: 0.8034\n",
            "  Epoch 5/10 - train_loss: 1.6599 - train_acc: 0.8010 - val_loss: 1.6407 - val_acc: 0.8204\n",
            "  Epoch 6/10 - train_loss: 1.6557 - train_acc: 0.8054 - val_loss: 1.6475 - val_acc: 0.8134\n",
            "  Epoch 7/10 - train_loss: 1.6640 - train_acc: 0.7966 - val_loss: 1.7055 - val_acc: 0.7546\n",
            "  Epoch 8/10 - train_loss: 1.6590 - train_acc: 0.8012 - val_loss: 1.6581 - val_acc: 0.8033\n",
            "  Epoch 9/10 - train_loss: 1.6612 - train_acc: 0.7993 - val_loss: 1.6940 - val_acc: 0.7661\n",
            "  Epoch 10/10 - train_loss: 1.6665 - train_acc: 0.7946 - val_loss: 1.6903 - val_acc: 0.7710\n",
            "Fold 3/3\n",
            "  Epoch 1/10 - train_loss: 1.7186 - train_acc: 0.7530 - val_loss: 1.6353 - val_acc: 0.8303\n",
            "  Epoch 2/10 - train_loss: 1.8302 - train_acc: 0.6297 - val_loss: 1.6963 - val_acc: 0.7646\n",
            "  Epoch 3/10 - train_loss: 1.6839 - train_acc: 0.7774 - val_loss: 1.6800 - val_acc: 0.7803\n",
            "  Epoch 4/10 - train_loss: 1.6760 - train_acc: 0.7847 - val_loss: 1.6846 - val_acc: 0.7757\n",
            "  Epoch 5/10 - train_loss: 1.6755 - train_acc: 0.7859 - val_loss: 1.6572 - val_acc: 0.8040\n",
            "  Epoch 6/10 - train_loss: 1.6633 - train_acc: 0.7975 - val_loss: 1.6494 - val_acc: 0.8114\n",
            "  Epoch 7/10 - train_loss: 1.6587 - train_acc: 0.8020 - val_loss: 1.6753 - val_acc: 0.7855\n",
            "  Epoch 8/10 - train_loss: 1.6563 - train_acc: 0.8046 - val_loss: 1.6374 - val_acc: 0.8235\n",
            "  Epoch 9/10 - train_loss: 1.6646 - train_acc: 0.7961 - val_loss: 1.6501 - val_acc: 0.8108\n",
            "  Epoch 10/10 - train_loss: 1.6553 - train_acc: 0.8054 - val_loss: 1.7750 - val_acc: 0.6842\n",
            "<---------------- Testing adamw with learning rate 0.1 and batch size 128 ---------------->\n",
            "Fold 1/3\n",
            "  Epoch 1/10 - train_loss: 1.6926 - train_acc: 0.7841 - val_loss: 1.5994 - val_acc: 0.8671\n",
            "  Epoch 2/10 - train_loss: 1.8367 - train_acc: 0.6236 - val_loss: 1.8123 - val_acc: 0.6472\n",
            "  Epoch 3/10 - train_loss: 1.8320 - train_acc: 0.6284 - val_loss: 1.7514 - val_acc: 0.7089\n",
            "  Epoch 4/10 - train_loss: 1.6922 - train_acc: 0.7680 - val_loss: 1.6659 - val_acc: 0.7941\n",
            "  Epoch 5/10 - train_loss: 1.6344 - train_acc: 0.8266 - val_loss: 1.6276 - val_acc: 0.8345\n",
            "  Epoch 6/10 - train_loss: 1.6196 - train_acc: 0.8419 - val_loss: 1.6206 - val_acc: 0.8402\n",
            "  Epoch 7/10 - train_loss: 1.6108 - train_acc: 0.8505 - val_loss: 1.6260 - val_acc: 0.8336\n",
            "  Epoch 8/10 - train_loss: 1.6090 - train_acc: 0.8520 - val_loss: 1.6208 - val_acc: 0.8417\n",
            "  Epoch 9/10 - train_loss: 1.6162 - train_acc: 0.8445 - val_loss: 1.6129 - val_acc: 0.8478\n",
            "  Epoch 10/10 - train_loss: 1.6171 - train_acc: 0.8439 - val_loss: 1.6132 - val_acc: 0.8479\n",
            "Fold 2/3\n",
            "  Epoch 1/10 - train_loss: 1.6927 - train_acc: 0.7819 - val_loss: 1.6160 - val_acc: 0.8477\n",
            "  Epoch 2/10 - train_loss: 1.8744 - train_acc: 0.5861 - val_loss: 2.0730 - val_acc: 0.3884\n",
            "  Epoch 3/10 - train_loss: 1.8412 - train_acc: 0.6190 - val_loss: 1.7916 - val_acc: 0.6687\n",
            "  Epoch 4/10 - train_loss: 1.7322 - train_acc: 0.7286 - val_loss: 1.6953 - val_acc: 0.7653\n",
            "  Epoch 5/10 - train_loss: 1.6849 - train_acc: 0.7760 - val_loss: 1.6863 - val_acc: 0.7750\n",
            "  Epoch 6/10 - train_loss: 1.6782 - train_acc: 0.7825 - val_loss: 1.6596 - val_acc: 0.8005\n",
            "  Epoch 7/10 - train_loss: 1.6637 - train_acc: 0.7974 - val_loss: 1.7062 - val_acc: 0.7541\n",
            "  Epoch 8/10 - train_loss: 1.6732 - train_acc: 0.7878 - val_loss: 1.6574 - val_acc: 0.8029\n",
            "  Epoch 9/10 - train_loss: 1.6619 - train_acc: 0.7986 - val_loss: 1.6628 - val_acc: 0.7984\n",
            "  Epoch 10/10 - train_loss: 1.6668 - train_acc: 0.7936 - val_loss: 1.6697 - val_acc: 0.7915\n",
            "Fold 3/3\n",
            "  Epoch 1/10 - train_loss: 1.6939 - train_acc: 0.7813 - val_loss: 1.6095 - val_acc: 0.8553\n",
            "  Epoch 2/10 - train_loss: 1.7341 - train_acc: 0.7263 - val_loss: 1.7197 - val_acc: 0.7405\n",
            "  Epoch 3/10 - train_loss: 1.6739 - train_acc: 0.7870 - val_loss: 1.6673 - val_acc: 0.7924\n",
            "  Epoch 4/10 - train_loss: 1.6229 - train_acc: 0.8379 - val_loss: 1.6095 - val_acc: 0.8519\n",
            "  Epoch 5/10 - train_loss: 1.6035 - train_acc: 0.8579 - val_loss: 1.6310 - val_acc: 0.8293\n",
            "  Epoch 6/10 - train_loss: 1.6060 - train_acc: 0.8547 - val_loss: 1.5998 - val_acc: 0.8620\n",
            "  Epoch 7/10 - train_loss: 1.6142 - train_acc: 0.8465 - val_loss: 1.6479 - val_acc: 0.8131\n",
            "  Epoch 8/10 - train_loss: 1.6041 - train_acc: 0.8567 - val_loss: 1.6103 - val_acc: 0.8511\n",
            "  Epoch 9/10 - train_loss: 1.6095 - train_acc: 0.8513 - val_loss: 1.6074 - val_acc: 0.8534\n",
            "  Epoch 10/10 - train_loss: 1.6091 - train_acc: 0.8518 - val_loss: 1.6411 - val_acc: 0.8196\n",
            "<---------------- Testing adamw with learning rate 0.01 and batch size 32 ---------------->\n",
            "Fold 1/3\n",
            "  Epoch 1/10 - train_loss: 1.7120 - train_acc: 0.7853 - val_loss: 1.5823 - val_acc: 0.8907\n",
            "  Epoch 2/10 - train_loss: 1.6411 - train_acc: 0.8205 - val_loss: 1.6240 - val_acc: 0.8365\n",
            "  Epoch 3/10 - train_loss: 1.6112 - train_acc: 0.8495 - val_loss: 1.6211 - val_acc: 0.8394\n",
            "  Epoch 4/10 - train_loss: 1.5912 - train_acc: 0.8696 - val_loss: 1.5774 - val_acc: 0.8834\n",
            "  Epoch 5/10 - train_loss: 1.5828 - train_acc: 0.8780 - val_loss: 1.5826 - val_acc: 0.8781\n",
            "  Epoch 6/10 - train_loss: 1.5787 - train_acc: 0.8819 - val_loss: 1.5764 - val_acc: 0.8850\n",
            "  Epoch 7/10 - train_loss: 1.5669 - train_acc: 0.8939 - val_loss: 1.5619 - val_acc: 0.8998\n",
            "  Epoch 8/10 - train_loss: 1.5617 - train_acc: 0.8996 - val_loss: 1.5612 - val_acc: 0.8996\n",
            "  Epoch 9/10 - train_loss: 1.5569 - train_acc: 0.9046 - val_loss: 1.5647 - val_acc: 0.8960\n",
            "  Epoch 10/10 - train_loss: 1.5553 - train_acc: 0.9059 - val_loss: 1.5600 - val_acc: 0.9013\n",
            "Fold 2/3\n",
            "  Epoch 1/10 - train_loss: 1.7151 - train_acc: 0.7817 - val_loss: 1.5847 - val_acc: 0.8873\n",
            "  Epoch 2/10 - train_loss: 1.6531 - train_acc: 0.8083 - val_loss: 1.6826 - val_acc: 0.7770\n",
            "  Epoch 3/10 - train_loss: 1.6123 - train_acc: 0.8478 - val_loss: 1.5941 - val_acc: 0.8661\n",
            "  Epoch 4/10 - train_loss: 1.5884 - train_acc: 0.8728 - val_loss: 1.5857 - val_acc: 0.8746\n",
            "  Epoch 5/10 - train_loss: 1.5820 - train_acc: 0.8792 - val_loss: 1.5904 - val_acc: 0.8701\n",
            "  Epoch 6/10 - train_loss: 1.5768 - train_acc: 0.8837 - val_loss: 1.5852 - val_acc: 0.8754\n",
            "  Epoch 7/10 - train_loss: 1.5684 - train_acc: 0.8927 - val_loss: 1.5719 - val_acc: 0.8891\n",
            "  Epoch 8/10 - train_loss: 1.5604 - train_acc: 0.9007 - val_loss: 1.5801 - val_acc: 0.8806\n",
            "  Epoch 9/10 - train_loss: 1.5616 - train_acc: 0.8989 - val_loss: 1.5625 - val_acc: 0.8992\n",
            "  Epoch 10/10 - train_loss: 1.5566 - train_acc: 0.9048 - val_loss: 1.5591 - val_acc: 0.9021\n",
            "Fold 3/3\n",
            "  Epoch 1/10 - train_loss: 1.7168 - train_acc: 0.7793 - val_loss: 1.5881 - val_acc: 0.8833\n",
            "  Epoch 2/10 - train_loss: 1.6499 - train_acc: 0.8111 - val_loss: 1.6252 - val_acc: 0.8339\n",
            "  Epoch 3/10 - train_loss: 1.6152 - train_acc: 0.8452 - val_loss: 1.6321 - val_acc: 0.8283\n",
            "  Epoch 4/10 - train_loss: 1.6014 - train_acc: 0.8591 - val_loss: 1.5780 - val_acc: 0.8827\n",
            "  Epoch 5/10 - train_loss: 1.5864 - train_acc: 0.8740 - val_loss: 1.6217 - val_acc: 0.8395\n",
            "  Epoch 6/10 - train_loss: 1.5801 - train_acc: 0.8802 - val_loss: 1.6121 - val_acc: 0.8486\n",
            "  Epoch 7/10 - train_loss: 1.5734 - train_acc: 0.8875 - val_loss: 1.5744 - val_acc: 0.8865\n",
            "  Epoch 8/10 - train_loss: 1.5667 - train_acc: 0.8943 - val_loss: 1.5638 - val_acc: 0.8971\n",
            "  Epoch 9/10 - train_loss: 1.5652 - train_acc: 0.8957 - val_loss: 1.5698 - val_acc: 0.8904\n",
            "  Epoch 10/10 - train_loss: 1.5606 - train_acc: 0.9004 - val_loss: 1.5636 - val_acc: 0.8974\n",
            "<---------------- Testing adamw with learning rate 0.01 and batch size 64 ---------------->\n",
            "Fold 1/3\n",
            "  Epoch 1/10 - train_loss: 1.6907 - train_acc: 0.8199 - val_loss: 1.5791 - val_acc: 0.8956\n",
            "  Epoch 2/10 - train_loss: 1.6279 - train_acc: 0.8337 - val_loss: 1.6423 - val_acc: 0.8183\n",
            "  Epoch 3/10 - train_loss: 1.6077 - train_acc: 0.8530 - val_loss: 1.6258 - val_acc: 0.8340\n",
            "  Epoch 4/10 - train_loss: 1.5969 - train_acc: 0.8636 - val_loss: 1.6028 - val_acc: 0.8575\n",
            "  Epoch 5/10 - train_loss: 1.5879 - train_acc: 0.8724 - val_loss: 1.5821 - val_acc: 0.8782\n",
            "  Epoch 6/10 - train_loss: 1.5856 - train_acc: 0.8748 - val_loss: 1.5638 - val_acc: 0.8967\n",
            "  Epoch 7/10 - train_loss: 1.5715 - train_acc: 0.8894 - val_loss: 1.5954 - val_acc: 0.8648\n",
            "  Epoch 8/10 - train_loss: 1.5684 - train_acc: 0.8921 - val_loss: 1.5778 - val_acc: 0.8828\n",
            "  Epoch 9/10 - train_loss: 1.5632 - train_acc: 0.8978 - val_loss: 1.5691 - val_acc: 0.8911\n",
            "  Epoch 10/10 - train_loss: 1.5639 - train_acc: 0.8972 - val_loss: 1.5663 - val_acc: 0.8945\n",
            "Fold 2/3\n",
            "  Epoch 1/10 - train_loss: 1.6931 - train_acc: 0.8163 - val_loss: 1.5782 - val_acc: 0.8970\n",
            "  Epoch 2/10 - train_loss: 1.6364 - train_acc: 0.8249 - val_loss: 1.6869 - val_acc: 0.7734\n",
            "  Epoch 3/10 - train_loss: 1.6059 - train_acc: 0.8547 - val_loss: 1.6057 - val_acc: 0.8549\n",
            "  Epoch 4/10 - train_loss: 1.5929 - train_acc: 0.8676 - val_loss: 1.6185 - val_acc: 0.8415\n",
            "  Epoch 5/10 - train_loss: 1.5916 - train_acc: 0.8690 - val_loss: 1.5990 - val_acc: 0.8622\n",
            "  Epoch 6/10 - train_loss: 1.5814 - train_acc: 0.8791 - val_loss: 1.5977 - val_acc: 0.8619\n",
            "  Epoch 7/10 - train_loss: 1.5712 - train_acc: 0.8896 - val_loss: 1.5930 - val_acc: 0.8679\n",
            "  Epoch 8/10 - train_loss: 1.5747 - train_acc: 0.8858 - val_loss: 1.5715 - val_acc: 0.8892\n",
            "  Epoch 9/10 - train_loss: 1.5695 - train_acc: 0.8910 - val_loss: 1.5700 - val_acc: 0.8901\n",
            "  Epoch 10/10 - train_loss: 1.5659 - train_acc: 0.8946 - val_loss: 1.5771 - val_acc: 0.8829\n",
            "Fold 3/3\n",
            "  Epoch 1/10 - train_loss: 1.6931 - train_acc: 0.8161 - val_loss: 1.5750 - val_acc: 0.8986\n",
            "  Epoch 2/10 - train_loss: 1.6412 - train_acc: 0.8202 - val_loss: 1.6480 - val_acc: 0.8127\n",
            "  Epoch 3/10 - train_loss: 1.6132 - train_acc: 0.8469 - val_loss: 1.6122 - val_acc: 0.8483\n",
            "  Epoch 4/10 - train_loss: 1.5914 - train_acc: 0.8689 - val_loss: 1.5811 - val_acc: 0.8799\n",
            "  Epoch 5/10 - train_loss: 1.5893 - train_acc: 0.8712 - val_loss: 1.6128 - val_acc: 0.8480\n",
            "  Epoch 6/10 - train_loss: 1.5804 - train_acc: 0.8806 - val_loss: 1.5764 - val_acc: 0.8838\n",
            "  Epoch 7/10 - train_loss: 1.5748 - train_acc: 0.8861 - val_loss: 1.5830 - val_acc: 0.8770\n",
            "  Epoch 8/10 - train_loss: 1.5728 - train_acc: 0.8878 - val_loss: 1.5811 - val_acc: 0.8798\n",
            "  Epoch 9/10 - train_loss: 1.5672 - train_acc: 0.8940 - val_loss: 1.5744 - val_acc: 0.8865\n",
            "  Epoch 10/10 - train_loss: 1.5644 - train_acc: 0.8960 - val_loss: 1.5581 - val_acc: 0.9024\n",
            "<---------------- Testing adamw with learning rate 0.01 and batch size 128 ---------------->\n",
            "Fold 1/3\n",
            "  Epoch 1/10 - train_loss: 1.7026 - train_acc: 0.8256 - val_loss: 1.5821 - val_acc: 0.9034\n",
            "  Epoch 2/10 - train_loss: 1.6246 - train_acc: 0.8376 - val_loss: 1.6044 - val_acc: 0.8559\n",
            "  Epoch 3/10 - train_loss: 1.6012 - train_acc: 0.8592 - val_loss: 1.5960 - val_acc: 0.8642\n",
            "  Epoch 4/10 - train_loss: 1.5871 - train_acc: 0.8734 - val_loss: 1.5967 - val_acc: 0.8638\n",
            "  Epoch 5/10 - train_loss: 1.5801 - train_acc: 0.8804 - val_loss: 1.5894 - val_acc: 0.8712\n",
            "  Epoch 6/10 - train_loss: 1.5718 - train_acc: 0.8889 - val_loss: 1.5848 - val_acc: 0.8762\n",
            "  Epoch 7/10 - train_loss: 1.5683 - train_acc: 0.8924 - val_loss: 1.5729 - val_acc: 0.8879\n",
            "  Epoch 8/10 - train_loss: 1.5680 - train_acc: 0.8926 - val_loss: 1.5761 - val_acc: 0.8847\n",
            "  Epoch 9/10 - train_loss: 1.5655 - train_acc: 0.8954 - val_loss: 1.5778 - val_acc: 0.8832\n",
            "  Epoch 10/10 - train_loss: 1.5713 - train_acc: 0.8896 - val_loss: 1.5909 - val_acc: 0.8697\n",
            "Fold 2/3\n",
            "  Epoch 1/10 - train_loss: 1.6993 - train_acc: 0.8286 - val_loss: 1.5807 - val_acc: 0.9046\n",
            "  Epoch 2/10 - train_loss: 1.6316 - train_acc: 0.8300 - val_loss: 1.6302 - val_acc: 0.8305\n",
            "  Epoch 3/10 - train_loss: 1.6107 - train_acc: 0.8501 - val_loss: 1.6108 - val_acc: 0.8500\n",
            "  Epoch 4/10 - train_loss: 1.6018 - train_acc: 0.8586 - val_loss: 1.6032 - val_acc: 0.8569\n",
            "  Epoch 5/10 - train_loss: 1.5911 - train_acc: 0.8697 - val_loss: 1.5795 - val_acc: 0.8810\n",
            "  Epoch 6/10 - train_loss: 1.5870 - train_acc: 0.8733 - val_loss: 1.6029 - val_acc: 0.8570\n",
            "  Epoch 7/10 - train_loss: 1.5814 - train_acc: 0.8791 - val_loss: 1.5876 - val_acc: 0.8733\n",
            "  Epoch 8/10 - train_loss: 1.5848 - train_acc: 0.8760 - val_loss: 1.5891 - val_acc: 0.8717\n",
            "  Epoch 9/10 - train_loss: 1.5866 - train_acc: 0.8739 - val_loss: 1.5895 - val_acc: 0.8712\n",
            "  Epoch 10/10 - train_loss: 1.5740 - train_acc: 0.8869 - val_loss: 1.5864 - val_acc: 0.8742\n",
            "Fold 3/3\n",
            "  Epoch 1/10 - train_loss: 1.7040 - train_acc: 0.8262 - val_loss: 1.5829 - val_acc: 0.9013\n",
            "  Epoch 2/10 - train_loss: 1.6284 - train_acc: 0.8334 - val_loss: 1.6165 - val_acc: 0.8433\n",
            "  Epoch 3/10 - train_loss: 1.6047 - train_acc: 0.8559 - val_loss: 1.6104 - val_acc: 0.8503\n",
            "  Epoch 4/10 - train_loss: 1.6053 - train_acc: 0.8549 - val_loss: 1.6076 - val_acc: 0.8532\n",
            "  Epoch 5/10 - train_loss: 1.5869 - train_acc: 0.8736 - val_loss: 1.5848 - val_acc: 0.8761\n",
            "  Epoch 6/10 - train_loss: 1.5851 - train_acc: 0.8753 - val_loss: 1.5887 - val_acc: 0.8720\n",
            "  Epoch 7/10 - train_loss: 1.5757 - train_acc: 0.8850 - val_loss: 1.5698 - val_acc: 0.8916\n",
            "  Epoch 8/10 - train_loss: 1.5794 - train_acc: 0.8813 - val_loss: 1.5836 - val_acc: 0.8771\n",
            "  Epoch 9/10 - train_loss: 1.5730 - train_acc: 0.8877 - val_loss: 1.5614 - val_acc: 0.8997\n",
            "  Epoch 10/10 - train_loss: 1.5677 - train_acc: 0.8931 - val_loss: 1.5667 - val_acc: 0.8934\n",
            "<---------------- Testing adamw with learning rate 0.001 and batch size 32 ---------------->\n",
            "Fold 1/3\n",
            "  Epoch 1/10 - train_loss: 1.8261 - train_acc: 0.7602 - val_loss: 1.6419 - val_acc: 0.8808\n",
            "  Epoch 2/10 - train_loss: 1.5859 - train_acc: 0.8810 - val_loss: 1.5648 - val_acc: 0.8983\n",
            "  Epoch 3/10 - train_loss: 1.5544 - train_acc: 0.9083 - val_loss: 1.5538 - val_acc: 0.9083\n",
            "  Epoch 4/10 - train_loss: 1.5425 - train_acc: 0.9198 - val_loss: 1.5489 - val_acc: 0.9119\n",
            "  Epoch 5/10 - train_loss: 1.5346 - train_acc: 0.9273 - val_loss: 1.5406 - val_acc: 0.9204\n",
            "  Epoch 6/10 - train_loss: 1.5289 - train_acc: 0.9327 - val_loss: 1.5523 - val_acc: 0.9089\n",
            "  Epoch 7/10 - train_loss: 1.5250 - train_acc: 0.9364 - val_loss: 1.5377 - val_acc: 0.9230\n",
            "  Epoch 8/10 - train_loss: 1.5205 - train_acc: 0.9409 - val_loss: 1.5397 - val_acc: 0.9212\n",
            "  Epoch 9/10 - train_loss: 1.5185 - train_acc: 0.9425 - val_loss: 1.5349 - val_acc: 0.9263\n",
            "  Epoch 10/10 - train_loss: 1.5163 - train_acc: 0.9454 - val_loss: 1.5383 - val_acc: 0.9225\n",
            "Fold 2/3\n",
            "  Epoch 1/10 - train_loss: 1.8239 - train_acc: 0.7605 - val_loss: 1.6368 - val_acc: 0.8895\n",
            "  Epoch 2/10 - train_loss: 1.5843 - train_acc: 0.8820 - val_loss: 1.5647 - val_acc: 0.8984\n",
            "  Epoch 3/10 - train_loss: 1.5546 - train_acc: 0.9076 - val_loss: 1.5508 - val_acc: 0.9110\n",
            "  Epoch 4/10 - train_loss: 1.5403 - train_acc: 0.9223 - val_loss: 1.5612 - val_acc: 0.9008\n",
            "  Epoch 5/10 - train_loss: 1.5342 - train_acc: 0.9272 - val_loss: 1.5528 - val_acc: 0.9091\n",
            "  Epoch 6/10 - train_loss: 1.5303 - train_acc: 0.9316 - val_loss: 1.5367 - val_acc: 0.9248\n",
            "  Epoch 7/10 - train_loss: 1.5234 - train_acc: 0.9383 - val_loss: 1.5444 - val_acc: 0.9164\n",
            "  Epoch 8/10 - train_loss: 1.5197 - train_acc: 0.9416 - val_loss: 1.5370 - val_acc: 0.9240\n",
            "  Epoch 9/10 - train_loss: 1.5188 - train_acc: 0.9431 - val_loss: 1.5297 - val_acc: 0.9314\n",
            "  Epoch 10/10 - train_loss: 1.5172 - train_acc: 0.9441 - val_loss: 1.5253 - val_acc: 0.9358\n",
            "Fold 3/3\n",
            "  Epoch 1/10 - train_loss: 1.8256 - train_acc: 0.7615 - val_loss: 1.6367 - val_acc: 0.8849\n",
            "  Epoch 2/10 - train_loss: 1.5872 - train_acc: 0.8780 - val_loss: 1.5873 - val_acc: 0.8751\n",
            "  Epoch 3/10 - train_loss: 1.5575 - train_acc: 0.9048 - val_loss: 1.5521 - val_acc: 0.9100\n",
            "  Epoch 4/10 - train_loss: 1.5455 - train_acc: 0.9166 - val_loss: 1.5565 - val_acc: 0.9058\n",
            "  Epoch 5/10 - train_loss: 1.5347 - train_acc: 0.9271 - val_loss: 1.5369 - val_acc: 0.9244\n",
            "  Epoch 6/10 - train_loss: 1.5304 - train_acc: 0.9309 - val_loss: 1.5300 - val_acc: 0.9316\n",
            "  Epoch 7/10 - train_loss: 1.5276 - train_acc: 0.9339 - val_loss: 1.5408 - val_acc: 0.9205\n",
            "  Epoch 8/10 - train_loss: 1.5215 - train_acc: 0.9402 - val_loss: 1.5468 - val_acc: 0.9145\n",
            "  Epoch 9/10 - train_loss: 1.5203 - train_acc: 0.9413 - val_loss: 1.5409 - val_acc: 0.9204\n",
            "  Epoch 10/10 - train_loss: 1.5185 - train_acc: 0.9429 - val_loss: 1.5399 - val_acc: 0.9216\n",
            "<---------------- Testing adamw with learning rate 0.001 and batch size 64 ---------------->\n",
            "Fold 1/3\n",
            "  Epoch 1/10 - train_loss: 1.8422 - train_acc: 0.7705 - val_loss: 1.6818 - val_acc: 0.8804\n",
            "  Epoch 2/10 - train_loss: 1.5828 - train_acc: 0.8865 - val_loss: 1.5687 - val_acc: 0.8954\n",
            "  Epoch 3/10 - train_loss: 1.5492 - train_acc: 0.9146 - val_loss: 1.5570 - val_acc: 0.9061\n",
            "  Epoch 4/10 - train_loss: 1.5365 - train_acc: 0.9263 - val_loss: 1.5536 - val_acc: 0.9078\n",
            "  Epoch 5/10 - train_loss: 1.5273 - train_acc: 0.9355 - val_loss: 1.5386 - val_acc: 0.9233\n",
            "  Epoch 6/10 - train_loss: 1.5215 - train_acc: 0.9408 - val_loss: 1.5385 - val_acc: 0.9243\n",
            "  Epoch 7/10 - train_loss: 1.5163 - train_acc: 0.9459 - val_loss: 1.5276 - val_acc: 0.9344\n",
            "  Epoch 8/10 - train_loss: 1.5130 - train_acc: 0.9488 - val_loss: 1.5349 - val_acc: 0.9260\n",
            "  Epoch 9/10 - train_loss: 1.5120 - train_acc: 0.9495 - val_loss: 1.5398 - val_acc: 0.9206\n",
            "  Epoch 10/10 - train_loss: 1.5071 - train_acc: 0.9545 - val_loss: 1.5350 - val_acc: 0.9265\n",
            "Fold 2/3\n",
            "  Epoch 1/10 - train_loss: 1.8419 - train_acc: 0.7741 - val_loss: 1.6816 - val_acc: 0.8826\n",
            "  Epoch 2/10 - train_loss: 1.5847 - train_acc: 0.8840 - val_loss: 1.5793 - val_acc: 0.8857\n",
            "  Epoch 3/10 - train_loss: 1.5499 - train_acc: 0.9132 - val_loss: 1.5517 - val_acc: 0.9120\n",
            "  Epoch 4/10 - train_loss: 1.5357 - train_acc: 0.9277 - val_loss: 1.5517 - val_acc: 0.9111\n",
            "  Epoch 5/10 - train_loss: 1.5299 - train_acc: 0.9318 - val_loss: 1.5484 - val_acc: 0.9133\n",
            "  Epoch 6/10 - train_loss: 1.5207 - train_acc: 0.9419 - val_loss: 1.5326 - val_acc: 0.9287\n",
            "  Epoch 7/10 - train_loss: 1.5173 - train_acc: 0.9444 - val_loss: 1.5373 - val_acc: 0.9242\n",
            "  Epoch 8/10 - train_loss: 1.5144 - train_acc: 0.9476 - val_loss: 1.5313 - val_acc: 0.9310\n",
            "  Epoch 9/10 - train_loss: 1.5103 - train_acc: 0.9518 - val_loss: 1.5355 - val_acc: 0.9257\n",
            "  Epoch 10/10 - train_loss: 1.5081 - train_acc: 0.9537 - val_loss: 1.5432 - val_acc: 0.9185\n",
            "Fold 3/3\n",
            "  Epoch 1/10 - train_loss: 1.8449 - train_acc: 0.7672 - val_loss: 1.6785 - val_acc: 0.8814\n",
            "  Epoch 2/10 - train_loss: 1.5856 - train_acc: 0.8830 - val_loss: 1.5657 - val_acc: 0.8986\n",
            "  Epoch 3/10 - train_loss: 1.5537 - train_acc: 0.9098 - val_loss: 1.5536 - val_acc: 0.9093\n",
            "  Epoch 4/10 - train_loss: 1.5394 - train_acc: 0.9238 - val_loss: 1.5472 - val_acc: 0.9151\n",
            "  Epoch 5/10 - train_loss: 1.5308 - train_acc: 0.9316 - val_loss: 1.5437 - val_acc: 0.9185\n",
            "  Epoch 6/10 - train_loss: 1.5242 - train_acc: 0.9379 - val_loss: 1.5334 - val_acc: 0.9285\n",
            "  Epoch 7/10 - train_loss: 1.5205 - train_acc: 0.9412 - val_loss: 1.5345 - val_acc: 0.9271\n",
            "  Epoch 8/10 - train_loss: 1.5158 - train_acc: 0.9465 - val_loss: 1.5536 - val_acc: 0.9080\n",
            "  Epoch 9/10 - train_loss: 1.5116 - train_acc: 0.9504 - val_loss: 1.5312 - val_acc: 0.9308\n",
            "  Epoch 10/10 - train_loss: 1.5089 - train_acc: 0.9529 - val_loss: 1.5308 - val_acc: 0.9308\n",
            "<---------------- Testing adamw with learning rate 0.001 and batch size 128 ---------------->\n",
            "Fold 1/3\n",
            "  Epoch 1/10 - train_loss: 1.8809 - train_acc: 0.7523 - val_loss: 1.7394 - val_acc: 0.8683\n",
            "  Epoch 2/10 - train_loss: 1.5935 - train_acc: 0.8791 - val_loss: 1.5729 - val_acc: 0.8929\n",
            "  Epoch 3/10 - train_loss: 1.5551 - train_acc: 0.9101 - val_loss: 1.5525 - val_acc: 0.9120\n",
            "  Epoch 4/10 - train_loss: 1.5365 - train_acc: 0.9278 - val_loss: 1.5460 - val_acc: 0.9178\n",
            "  Epoch 5/10 - train_loss: 1.5288 - train_acc: 0.9349 - val_loss: 1.5475 - val_acc: 0.9149\n",
            "  Epoch 6/10 - train_loss: 1.5220 - train_acc: 0.9407 - val_loss: 1.5359 - val_acc: 0.9263\n",
            "  Epoch 7/10 - train_loss: 1.5151 - train_acc: 0.9479 - val_loss: 1.5331 - val_acc: 0.9283\n",
            "  Epoch 8/10 - train_loss: 1.5106 - train_acc: 0.9520 - val_loss: 1.5282 - val_acc: 0.9327\n",
            "  Epoch 9/10 - train_loss: 1.5070 - train_acc: 0.9559 - val_loss: 1.5316 - val_acc: 0.9305\n",
            "  Epoch 10/10 - train_loss: 1.5045 - train_acc: 0.9579 - val_loss: 1.5306 - val_acc: 0.9305\n",
            "Fold 2/3\n",
            "  Epoch 1/10 - train_loss: 1.8809 - train_acc: 0.7556 - val_loss: 1.7402 - val_acc: 0.8712\n",
            "  Epoch 2/10 - train_loss: 1.5939 - train_acc: 0.8776 - val_loss: 1.5641 - val_acc: 0.9012\n",
            "  Epoch 3/10 - train_loss: 1.5526 - train_acc: 0.9131 - val_loss: 1.5506 - val_acc: 0.9136\n",
            "  Epoch 4/10 - train_loss: 1.5384 - train_acc: 0.9256 - val_loss: 1.5512 - val_acc: 0.9123\n",
            "  Epoch 5/10 - train_loss: 1.5259 - train_acc: 0.9376 - val_loss: 1.5362 - val_acc: 0.9273\n",
            "  Epoch 6/10 - train_loss: 1.5198 - train_acc: 0.9434 - val_loss: 1.5361 - val_acc: 0.9271\n",
            "  Epoch 7/10 - train_loss: 1.5138 - train_acc: 0.9492 - val_loss: 1.5325 - val_acc: 0.9295\n",
            "  Epoch 8/10 - train_loss: 1.5105 - train_acc: 0.9525 - val_loss: 1.5270 - val_acc: 0.9350\n",
            "  Epoch 9/10 - train_loss: 1.5064 - train_acc: 0.9560 - val_loss: 1.5301 - val_acc: 0.9313\n",
            "  Epoch 10/10 - train_loss: 1.5040 - train_acc: 0.9588 - val_loss: 1.5232 - val_acc: 0.9389\n",
            "Fold 3/3\n",
            "  Epoch 1/10 - train_loss: 1.8822 - train_acc: 0.7504 - val_loss: 1.7403 - val_acc: 0.8656\n",
            "  Epoch 2/10 - train_loss: 1.5927 - train_acc: 0.8810 - val_loss: 1.5658 - val_acc: 0.9003\n",
            "  Epoch 3/10 - train_loss: 1.5527 - train_acc: 0.9126 - val_loss: 1.5621 - val_acc: 0.9030\n",
            "  Epoch 4/10 - train_loss: 1.5382 - train_acc: 0.9258 - val_loss: 1.5516 - val_acc: 0.9113\n",
            "  Epoch 5/10 - train_loss: 1.5277 - train_acc: 0.9354 - val_loss: 1.5427 - val_acc: 0.9210\n",
            "  Epoch 6/10 - train_loss: 1.5214 - train_acc: 0.9417 - val_loss: 1.5341 - val_acc: 0.9284\n",
            "  Epoch 7/10 - train_loss: 1.5162 - train_acc: 0.9467 - val_loss: 1.5275 - val_acc: 0.9346\n",
            "  Epoch 8/10 - train_loss: 1.5096 - train_acc: 0.9533 - val_loss: 1.5258 - val_acc: 0.9362\n",
            "  Epoch 9/10 - train_loss: 1.5075 - train_acc: 0.9555 - val_loss: 1.5276 - val_acc: 0.9331\n",
            "  Epoch 10/10 - train_loss: 1.5061 - train_acc: 0.9564 - val_loss: 1.5307 - val_acc: 0.9301\n"
          ]
        }
      ],
      "source": [
        "# Iterate over optimizers, learning rates, and batch sizes\n",
        "results = []\n",
        "\n",
        "for optimizer_name in ['adam', 'rmsprop', 'adamw']:\n",
        "    for lr in learning_rates:\n",
        "        for bs in batch_sizes:\n",
        "            print(f\"<---------------- Testing {optimizer_name} with learning rate {lr} and batch size {bs} ---------------->\")\n",
        "            accuracy, loss, training_time = train_and_evaluate(optimizer_name, lr, bs)\n",
        "            results.append((optimizer_name, lr, bs, loss, accuracy, training_time))  # Save results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFCEQ1VVVCL3",
        "outputId": "43174b62-4de8-4459-c93c-71d04f78be76"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Optimizer</th>\n",
              "      <th>Learning Rate</th>\n",
              "      <th>Batch Size</th>\n",
              "      <th>Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Training Time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>adam</td>\n",
              "      <td>0.100</td>\n",
              "      <td>32</td>\n",
              "      <td>2.284238</td>\n",
              "      <td>0.176917</td>\n",
              "      <td>210.872395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>adam</td>\n",
              "      <td>0.100</td>\n",
              "      <td>64</td>\n",
              "      <td>2.361175</td>\n",
              "      <td>0.100033</td>\n",
              "      <td>170.714059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>adam</td>\n",
              "      <td>0.100</td>\n",
              "      <td>128</td>\n",
              "      <td>2.337560</td>\n",
              "      <td>0.123483</td>\n",
              "      <td>149.701033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>adam</td>\n",
              "      <td>0.010</td>\n",
              "      <td>32</td>\n",
              "      <td>1.782198</td>\n",
              "      <td>0.678950</td>\n",
              "      <td>213.197458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>adam</td>\n",
              "      <td>0.010</td>\n",
              "      <td>64</td>\n",
              "      <td>1.831122</td>\n",
              "      <td>0.630050</td>\n",
              "      <td>162.162949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>adam</td>\n",
              "      <td>0.010</td>\n",
              "      <td>128</td>\n",
              "      <td>1.695504</td>\n",
              "      <td>0.765333</td>\n",
              "      <td>146.122388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>adam</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>1.538020</td>\n",
              "      <td>0.922683</td>\n",
              "      <td>200.292868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>adam</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>1.534908</td>\n",
              "      <td>0.926183</td>\n",
              "      <td>166.860024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>adam</td>\n",
              "      <td>0.001</td>\n",
              "      <td>128</td>\n",
              "      <td>1.529555</td>\n",
              "      <td>0.932033</td>\n",
              "      <td>149.823922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>rmsprop</td>\n",
              "      <td>0.100</td>\n",
              "      <td>32</td>\n",
              "      <td>1.840096</td>\n",
              "      <td>0.621050</td>\n",
              "      <td>207.823540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>rmsprop</td>\n",
              "      <td>0.100</td>\n",
              "      <td>64</td>\n",
              "      <td>2.034961</td>\n",
              "      <td>0.426233</td>\n",
              "      <td>162.315227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>rmsprop</td>\n",
              "      <td>0.100</td>\n",
              "      <td>128</td>\n",
              "      <td>1.739346</td>\n",
              "      <td>0.721617</td>\n",
              "      <td>147.539476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>rmsprop</td>\n",
              "      <td>0.010</td>\n",
              "      <td>32</td>\n",
              "      <td>1.687027</td>\n",
              "      <td>0.774067</td>\n",
              "      <td>201.474209</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>rmsprop</td>\n",
              "      <td>0.010</td>\n",
              "      <td>64</td>\n",
              "      <td>1.635734</td>\n",
              "      <td>0.825167</td>\n",
              "      <td>158.773749</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>rmsprop</td>\n",
              "      <td>0.010</td>\n",
              "      <td>128</td>\n",
              "      <td>1.617818</td>\n",
              "      <td>0.843033</td>\n",
              "      <td>147.919724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>rmsprop</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>1.546214</td>\n",
              "      <td>0.914683</td>\n",
              "      <td>199.816072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>rmsprop</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>1.531430</td>\n",
              "      <td>0.929483</td>\n",
              "      <td>157.421359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>rmsprop</td>\n",
              "      <td>0.001</td>\n",
              "      <td>128</td>\n",
              "      <td>1.526891</td>\n",
              "      <td>0.934417</td>\n",
              "      <td>145.459054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>adamw</td>\n",
              "      <td>0.100</td>\n",
              "      <td>32</td>\n",
              "      <td>1.674757</td>\n",
              "      <td>0.787050</td>\n",
              "      <td>206.509612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>adamw</td>\n",
              "      <td>0.100</td>\n",
              "      <td>64</td>\n",
              "      <td>1.711072</td>\n",
              "      <td>0.749150</td>\n",
              "      <td>161.605730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>adamw</td>\n",
              "      <td>0.100</td>\n",
              "      <td>128</td>\n",
              "      <td>1.641311</td>\n",
              "      <td>0.819683</td>\n",
              "      <td>150.454213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>adamw</td>\n",
              "      <td>0.010</td>\n",
              "      <td>32</td>\n",
              "      <td>1.560902</td>\n",
              "      <td>0.900250</td>\n",
              "      <td>207.002294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>adamw</td>\n",
              "      <td>0.010</td>\n",
              "      <td>64</td>\n",
              "      <td>1.567168</td>\n",
              "      <td>0.893283</td>\n",
              "      <td>168.504872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>adamw</td>\n",
              "      <td>0.010</td>\n",
              "      <td>128</td>\n",
              "      <td>1.581330</td>\n",
              "      <td>0.879083</td>\n",
              "      <td>148.277516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>adamw</td>\n",
              "      <td>0.001</td>\n",
              "      <td>32</td>\n",
              "      <td>1.534497</td>\n",
              "      <td>0.926650</td>\n",
              "      <td>205.328778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>adamw</td>\n",
              "      <td>0.001</td>\n",
              "      <td>64</td>\n",
              "      <td>1.536347</td>\n",
              "      <td>0.925283</td>\n",
              "      <td>162.605395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>adamw</td>\n",
              "      <td>0.001</td>\n",
              "      <td>128</td>\n",
              "      <td>1.528170</td>\n",
              "      <td>0.933200</td>\n",
              "      <td>148.415535</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Optimizer  Learning Rate  Batch Size      Loss  Accuracy  Training Time\n",
              "0       adam          0.100          32  2.284238  0.176917     210.872395\n",
              "1       adam          0.100          64  2.361175  0.100033     170.714059\n",
              "2       adam          0.100         128  2.337560  0.123483     149.701033\n",
              "3       adam          0.010          32  1.782198  0.678950     213.197458\n",
              "4       adam          0.010          64  1.831122  0.630050     162.162949\n",
              "5       adam          0.010         128  1.695504  0.765333     146.122388\n",
              "6       adam          0.001          32  1.538020  0.922683     200.292868\n",
              "7       adam          0.001          64  1.534908  0.926183     166.860024\n",
              "8       adam          0.001         128  1.529555  0.932033     149.823922\n",
              "9    rmsprop          0.100          32  1.840096  0.621050     207.823540\n",
              "10   rmsprop          0.100          64  2.034961  0.426233     162.315227\n",
              "11   rmsprop          0.100         128  1.739346  0.721617     147.539476\n",
              "12   rmsprop          0.010          32  1.687027  0.774067     201.474209\n",
              "13   rmsprop          0.010          64  1.635734  0.825167     158.773749\n",
              "14   rmsprop          0.010         128  1.617818  0.843033     147.919724\n",
              "15   rmsprop          0.001          32  1.546214  0.914683     199.816072\n",
              "16   rmsprop          0.001          64  1.531430  0.929483     157.421359\n",
              "17   rmsprop          0.001         128  1.526891  0.934417     145.459054\n",
              "18     adamw          0.100          32  1.674757  0.787050     206.509612\n",
              "19     adamw          0.100          64  1.711072  0.749150     161.605730\n",
              "20     adamw          0.100         128  1.641311  0.819683     150.454213\n",
              "21     adamw          0.010          32  1.560902  0.900250     207.002294\n",
              "22     adamw          0.010          64  1.567168  0.893283     168.504872\n",
              "23     adamw          0.010         128  1.581330  0.879083     148.277516\n",
              "24     adamw          0.001          32  1.534497  0.926650     205.328778\n",
              "25     adamw          0.001          64  1.536347  0.925283     162.605395\n",
              "26     adamw          0.001         128  1.528170  0.933200     148.415535"
            ]
          },
          "execution_count": 134,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Convert results to a DataFrame for comparison\n",
        "results_df = pd.DataFrame(results, columns=['Optimizer', 'Learning Rate', 'Batch Size', \"Loss\", 'Accuracy', \"Training Time\"])\n",
        "results_df.to_csv('Optimizer_Results.csv')\n",
        "results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Optimizer  Learning Rate  Batch Size      Loss  Accuracy  Training Time\n",
            "8       adam          0.001         128  1.529555  0.932033     149.823922\n",
            "26     adamw          0.001         128  1.528170  0.933200     148.415535\n",
            "17   rmsprop          0.001         128  1.526891  0.934417     145.459054\n"
          ]
        }
      ],
      "source": [
        "# Find the best results for each optimizer\n",
        "best_results = results_df.loc[results_df.groupby('Optimizer')['Accuracy'].idxmax()]\n",
        "\n",
        "# Display best results\n",
        "print(best_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [],
      "source": [
        "bar_colors = ['skyblue', 'salmon', 'lightgreen']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "plt.bar(best_results['Optimizer'], best_results['Accuracy'], color=bar_colors)\n",
        "# Display the graphs\n",
        "plt.xlabel('Optimizer')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy comparison')\n",
        "plt.ylim([0.9, 0.95])\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "plt.bar(best_results['Optimizer'], best_results['Loss'], color=bar_colors)\n",
        "# Display the graphs\n",
        "plt.xlabel('Optimizer')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss comparison')\n",
        "plt.grid(True)\n",
        "plt.ylim([1.2, 1.6])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAHHCAYAAABZbpmkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABHE0lEQVR4nO3deVwVZf//8fc5gCAqIqggBahp7ihfvHPfksSlRbPUOys100oxlexWM9cs1MrMPbv76rf71upXLt2Z4r60uKLepqlpuZUhJSoqisCZ3x/dnrsjWng4ODC8no+Hj4dzzZzr+sxw6byZmXOOzTAMQwAAABZlN7sAAACAgkTYAQAAlkbYAQAAlkbYAQAAlkbYAQAAlkbYAQAAlkbYAQAAlkbYAQAAlkbYAQAAlkbYAQqJ3r17q3LlygU+js1m07hx4wp8HHctWLBANptNx44dM7sUqPDPFyAvCDvAn7DZbHn6s3HjRrNLLbRat26dp2PISRVAQbDx3VjAH/vnP//psvz+++9rzZo1+sc//uHSft999ykkJMTtcbKysuRwOOTr6+t2H3lx5coVeXt7y9vbu0DH+b01a9bo9OnTzuUdO3Zo+vTpeumll1SrVi1ne1RUlOrUqaOsrCz5+vrKZrPdthpxY2bMF8DTCDvALYqPj9esWbP0Z/90MjIy5O/vf5uqKlo++eQTPfroo9qwYYNat25tdjm4jsPh0NWrV+Xn52d2KYBHcBsL8IDWrVurbt26Sk5OVsuWLeXv76+XXnpJkvTpp5+qU6dOCgsLk6+vr+666y698sorysnJcenj+md2jh07JpvNpjfeeEPz5s3TXXfdJV9fX/3lL3/Rjh07cr22dOnS+umnn9S5c2eVLl1aFSpU0LBhw3KNc/3tonHjxslms+nIkSPq3bu3AgMDVbZsWfXp00cZGRkur718+bKef/55lS9fXmXKlNGDDz6on376yaO3oG70zE7lypV1//33a+PGjWrYsKFKliypevXqOW8dLlmyRPXq1ZOfn59iYmK0e/fuXP0ePHhQjzzyiIKCguTn56eGDRvqX//6V55qcjgcevvtt51jVKhQQe3bt9fOnTud22RnZ+uVV15x/pwqV66sl156SZmZmS595Xdfrv2sf/jhB8XFxalUqVIKCwvThAkTcgXwN954Q02bNlVwcLBKliypmJgYffLJJ7n2z2azKT4+XgsXLlSdOnXk6+urpKQk57rf/2wvXLigIUOGqHLlyvL19VXFihV13333adeuXS59fvzxx4qJiVHJkiVVvnx5Pf744/rpp59uuC95mbdAfhB2AA85c+aMOnTooAYNGmjatGlq06aNpN9O3qVLl1ZCQoLefvttxcTEaMyYMRoxYkSe+l20aJFef/11PfPMM5o4caKOHTumhx9+WFlZWS7b5eTkKC4uTsHBwXrjjTfUqlUrvfnmm5o3b16exunWrZsuXLigxMREdevWTQsWLND48eNdtundu7dmzJihjh07avLkySpZsqQ6deqUp/7z68iRI3rsscf0wAMPKDExUWfPntUDDzyghQsXaujQoXr88cc1fvx4ff/99+rWrZscDofztfv371fjxo114MABjRgxQm+++aZKlSqlzp07a+nSpX86dt++fTVkyBCFh4dr8uTJGjFihPz8/LR161bnNk8//bTGjBmj//mf/9Fbb72lVq1aKTExUT169PDovki//azbt2+vkJAQTZkyRTExMRo7dqzGjh3rst3bb7+t6OhoTZgwQa+99pq8vb316KOP6vPPP89V0/r16zV06FB1795db7/99k0fln/22Wc1Z84cde3aVbNnz9awYcNUsmRJHThwwLnNggUL1K1bN3l5eSkxMVH9+vXTkiVL1Lx5c507dy7XvuRn3gJ5YgC4JQMHDjSu/6fTqlUrQ5Ixd+7cXNtnZGTkanvmmWcMf39/48qVK862Xr16GZGRkc7lo0ePGpKM4OBgIy0tzdn+6aefGpKMzz77zOW1kowJEya4jBMdHW3ExMS4tEkyxo4d61weO3asIcl46qmnXLbr0qWLERwc7FxOTk42JBlDhgxx2a537965+vwzH3/8sSHJ2LBhQ6518+fPNyQZR48edbZFRkYakoyvv/7a2bZq1SpDklGyZEnj+PHjzvZ33nknV99t27Y16tWr53K8HQ6H0bRpU6N69ep/WOv69esNScbzzz+fa53D4TAMwzD27NljSDKefvppl/XDhg0zJBnr16/32L5c+1kPGjTIpY5OnToZJUqUMH755Rdn+/Vz7+rVq0bdunWNe++916VdkmG32439+/fn2sfrf7Zly5Y1Bg4cmGu7349RsWJFo27dusbly5ed7cuXLzckGWPGjMm1L3mZt0B+cGUH8BBfX1/16dMnV3vJkiWdf79w4YJ+/fVXtWjRQhkZGTp48OCf9tu9e3eVK1fOudyiRQtJ0g8//JBr22effdZluUWLFjfc7kZu9NozZ84oPT1dkpy3NQYMGOCy3aBBg/LUf37Vrl1bTZo0cS43atRIknTvvfcqIiIiV/u1/U5LS9P69eudV65+/fVX/frrrzpz5ozi4uJ0+PDhXLdXfm/x4sWy2Wy5rppIcj5AvWLFCklSQkKCy/oXXnhBknJdSXF3X34vPj7epY74+HhdvXpVa9eudbb/fu6dPXtW58+fV4sWLXLdcpKkVq1aqXbt2rnarxcYGKht27bp1KlTN1y/c+dOpaamasCAAS7P/HTq1Ek1a9a84VWl/MxbIC8IO4CH3HHHHSpRokSu9v3796tLly4qW7asAgICVKFCBT3++OOSpPPnz/9pv78/+UlyBp+zZ8+6tF97luT6ba/fzt1xjh8/LrvdripVqrhsV61atTz1n1/X11e2bFlJUnh4+A3br9V95MgRGYah0aNHq0KFCi5/rgWY1NTUm477/fffKywsTEFBQTfd5tqxuf5YhIaGKjAwUMePH/fIvlxjt9tVtWpVl7a7775bklyedVq+fLkaN24sPz8/BQUFqUKFCpozZ84N5931P9ebmTJlivbt26fw8HDdc889GjdunEswubavNWrUyPXamjVr5joW+Z23QF7wXkLAQ37/W/Q1586dU6tWrRQQEKAJEyborrvukp+fn3bt2qXhw4fnehbjRry8vG7Yblz3MOrNtsurvI5jlpvV92d1XzvGw4YNU1xc3A239VRgy+tb5d3dl1vxxRdf6MEHH1TLli01e/ZsVapUST4+Ppo/f74WLVqUa/sbzd8b6datm1q0aKGlS5dq9erVev311zV58mQtWbJEHTp0uOU68ztvgbwg7AAFaOPGjTpz5oyWLFmili1bOtuPHj1qYlXuiYyMlMPh0NGjR1W9enVn+5EjR0ys6s9duwLi4+Oj2NjYW379XXfdpVWrViktLe2mV3euHZvDhw+7fG7Q6dOnde7cOUVGRrpX/E04HA798MMPzqs5kvTdd99JkvPB4sWLF8vPz0+rVq1y+eym+fPn53v8SpUqacCAARowYIBSU1P1P//zP3r11VfVoUMH574eOnRI9957r8vrDh065PFjAeQFt7GAAnTtt9bf/2Z+9epVzZ4926yS3Hbtqsj1tc+YMcOMcvKsYsWKat26td555x39/PPPudb/8ssvf/j6rl27yjCMXO9Mk/77c+3YsaMkadq0aS7rp06dKkkF8o61mTNnutQxc+ZM+fj4qG3btpJ+m3s2m83lLdzHjh3TsmXL3B4zJycn1y2wihUrKiwszPkW+4YNG6pixYqaO3euy9vuV65cqQMHDty2d+8Bv8eVHaAANW3aVOXKlVOvXr30/PPPy2az6R//+EehuTV0K2JiYtS1a1dNmzZNZ86cUePGjbVp0ybnFYXC/GnHs2bNUvPmzVWvXj3169dPVatW1enTp7Vlyxb9+OOP+ve//33T17Zp00ZPPPGEpk+frsOHD6t9+/ZyOBz64osv1KZNG8XHx6t+/frq1auX5s2b57x1uX37dv3f//2fOnfu7PwYAk/x8/NTUlKSevXqpUaNGmnlypX6/PPP9dJLLzmff+nUqZOmTp2q9u3b67HHHlNqaqpmzZqlatWqae/evW6Ne+HCBd1555165JFHVL9+fZUuXVpr167Vjh079Oabb0r67Qra5MmT1adPH7Vq1Up//etfdfr0aefb2YcOHeqx4wDkFWEHKEDBwcFavny5XnjhBb388ssqV66cHn/8cbVt2/amz48UZu+//75CQ0P1wQcfaOnSpYqNjdVHH32kGjVqFOpP261du7Z27typ8ePHa8GCBTpz5owqVqyo6OhojRkz5k9fP3/+fEVFRem9997Tiy++qLJly6phw4Zq2rSpc5u///3vqlq1qhYsWKClS5cqNDRUI0eOvOG7uPLLy8tLSUlJeu655/Tiiy+qTJkyGjt2rMu+3HvvvXrvvfc0adIkDRkyRFWqVNHkyZN17Ngxt8OOv7+/BgwYoNWrV2vJkiVyOByqVq2aZs+ereeee865Xe/eveXv769JkyZp+PDhKlWqlLp06aLJkycrMDAwv7sP3DK+LgJAvuzZs0fR0dH65z//qZ49e5pdjuX17t1bn3zyiS5evGh2KUCRwTM7APLs8uXLudqmTZsmu93u8gA2ABQm3MYCkGdTpkxRcnKy2rRpI29vb61cuVIrV65U//79c31GDAAUFoQdAHnWtGlTrVmzRq+88oouXryoiIgIjRs3TqNGjTK7NAC4KZ7ZAQAAlsYzOwAAwNIIOwAAwNJ4Zke/ffT6qVOnVKZMmUL9wWgAAOC/DMPQhQsXFBYWJrv95tdvCDuSTp06xTtJAAAook6ePKk777zzpusJO5LKlCkj6beDFRAQYHI11pKVlaXVq1erXbt28vHxMbscFEPMQZiNOVhw0tPTFR4e7jyP3wxhR//9Tp+AgADCjodlZWXJ399fAQEB/COHKZiDMBtzsOD92SMoPKAMAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAsjbADAAAszdvsAqxu0u5fzS7BVHZHtmpIemvvGTnsxXO6jYgub3YJAFCscWUHAABYGmEHAABYWvG8rwAUI1njXzC7BFNl2b2kqGbKmjRKcuSYXY4pfMa+aXYJgKm4sgMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNt54DAArU22ffNrsEU9mybYpUpOacmyPD2zC7HFMMLjfY1PG5sgMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACyNsAMAACzN1LCzefNmPfDAAwoLC5PNZtOyZctuuu2zzz4rm82madOmubSnpaWpZ8+eCggIUGBgoPr27auLFy8WbOEAAKDIMDXsXLp0SfXr19esWbP+cLulS5dq69atCgsLy7WuZ8+e2r9/v9asWaPly5dr8+bN6t+/f0GVDAAAihhTvy6iQ4cO6tChwx9u89NPP2nQoEFatWqVOnXq5LLuwIEDSkpK0o4dO9SwYUNJ0owZM9SxY0e98cYbNwxHAACgeCnU343lcDj0xBNP6MUXX1SdOnVyrd+yZYsCAwOdQUeSYmNjZbfbtW3bNnXp0uWG/WZmZiozM9O5nJ6eLknKyspSVlaWR/fB7sj2aH9FzbX9L87HwdNz6pbHt3uZOr7Zsv+z/9nF+TiYPAdt2TZTxzfbtf0vzsehoP4fzGu/hTrsTJ48Wd7e3nr++edvuD4lJUUVK1Z0afP29lZQUJBSUlJu2m9iYqLGjx+fq3316tXy9/fPX9HXqeHR3oqu6qeSzS7BNCt+NLmAqGYmF1A4bKjb2OwSzLNihanDRyrS1PELi4jtEWaXYJoVKpg5mJGRkaftCm3YSU5O1ttvv61du3bJZvNsGh45cqQSEhKcy+np6QoPD1e7du0UEBDg0bHe2nvGo/0VNXZHtqqfStbhsBg57IV2uhWooVHBpo6fNWmUqeObLdvupQ11G6vNvq3yduSYXY4pfEa8aur4c87NMXV8s9mybYrYHqET95wott96/lzgcwXS77U7M3+m0J59vvjiC6Wmpioi4r9JOCcnRy+88IKmTZumY8eOKTQ0VKmpqS6vy87OVlpamkJDQ2/at6+vr3x9fXO1+/j4yMfHx3M7IRXbE/z1HHbvYnssPD2nblkxPcFfz9uRI59ieizMnoPF9QR/PcPbKLbHoqDmYF77LbRnnyeeeEKxsbEubXFxcXriiSfUp08fSVKTJk107tw5JScnKyYmRpK0fv16ORwONWrU6LbXDAAACh9Tw87Fixd15MgR5/LRo0e1Z88eBQUFKSIiQsHBrpf/fXx8FBoaqho1fnsSplatWmrfvr369eunuXPnKisrS/Hx8erRowfvxAIAAJJM/pydnTt3Kjo6WtHR0ZKkhIQERUdHa8yYMXnuY+HChapZs6batm2rjh07qnnz5po3b15BlQwAAIoYU6/stG7dWoaR9/uXx44dy9UWFBSkRYsWebAqAABgJXw3FgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDTCDgAAsDRTw87mzZv1wAMPKCwsTDabTcuWLXOuy8rK0vDhw1WvXj2VKlVKYWFhevLJJ3Xq1CmXPtLS0tSzZ08FBAQoMDBQffv21cWLF2/zngAAgMLK1LBz6dIl1a9fX7Nmzcq1LiMjQ7t27dLo0aO1a9cuLVmyRIcOHdKDDz7osl3Pnj21f/9+rVmzRsuXL9fmzZvVv3//27ULAACgkPM2c/AOHTqoQ4cON1xXtmxZrVmzxqVt5syZuueee3TixAlFRETowIEDSkpK0o4dO9SwYUNJ0owZM9SxY0e98cYbCgsLK/B9AAAAhZupYedWnT9/XjabTYGBgZKkLVu2KDAw0Bl0JCk2NlZ2u13btm1Tly5dbthPZmamMjMzncvp6emSfrt1lpWV5dGa7Y5sj/ZX1Fzb/+J8HDw9p255fLuXqeObLfs/+59dnI+DyXPQlm0zdXyzXdv/4nwcCur/wbz2W2TCzpUrVzR8+HD99a9/VUBAgCQpJSVFFStWdNnO29tbQUFBSklJuWlfiYmJGj9+fK721atXy9/f36N11/Bob0VX9VPJZpdgmhU/mlxAVDOTCygcNtRtbHYJ5lmxwtThIxVp6viFRcT2CLNLMM0KFcwczMjIyNN2RSLsZGVlqVu3bjIMQ3PmzMl3fyNHjlRCQoJzOT09XeHh4WrXrp0zSHnKW3vPeLS/osbuyFb1U8k6HBYjh71ITDePGxoVbOr4WZNGmTq+2bLtXtpQt7Ha7Nsqb0eO2eWYwmfEq6aOP+dc/v/fLsps2TZFbI/QiXtOyPA2zC7HFM8FPlcg/V67M/NnCv3Z51rQOX78uNavX+8SRkJDQ5WamuqyfXZ2ttLS0hQaGnrTPn19feXr65ur3cfHRz4+Pp4rXiq2J/jrOezexfZYeHpO3bJieoK/nrcjRz7F9FiYPQeL6wn+eoa3UWyPRUHNwbz2W6g/Z+da0Dl8+LDWrl2r4GDX35CbNGmic+fOKTn5v7dI1q9fL4fDoUaNGt3ucgEAQCFk6q/aFy9e1JEjR5zLR48e1Z49exQUFKRKlSrpkUce0a5du7R8+XLl5OQ4n8MJCgpSiRIlVKtWLbVv3179+vXT3LlzlZWVpfj4ePXo0YN3YgEAAEkmh52dO3eqTZs2zuVrz9H06tVL48aN07/+9S9JUoMGDVxet2HDBrVu3VqStHDhQsXHx6tt27ay2+3q2rWrpk+fflvqBwAAhZ+pYad169YyjJvfv/yjddcEBQVp0aJFniwLAABYSKF+ZgcAACC/CDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSCDsAAMDSTA07mzdv1gMPPKCwsDDZbDYtW7bMZb1hGBozZowqVaqkkiVLKjY2VocPH3bZJi0tTT179lRAQIACAwPVt29fXbx48TbuBQAAKMxMDTuXLl1S/fr1NWvWrBuunzJliqZPn665c+dq27ZtKlWqlOLi4nTlyhXnNj179tT+/fu1Zs0aLV++XJs3b1b//v1v1y4AAIBCztvMwTt06KAOHTrccJ1hGJo2bZpefvllPfTQQ5Kk999/XyEhIVq2bJl69OihAwcOKCkpSTt27FDDhg0lSTNmzFDHjh31xhtvKCws7LbtCwAAKJwK7TM7R48eVUpKimJjY51tZcuWVaNGjbRlyxZJ0pYtWxQYGOgMOpIUGxsru92ubdu23faaAQBA4ePWlZ2jR4/qiy++0PHjx5WRkaEKFSooOjpaTZo0kZ+fn0cKS0lJkSSFhIS4tIeEhDjXpaSkqGLFii7rvb29FRQU5NzmRjIzM5WZmelcTk9PlyRlZWUpKyvLI/VfY3dke7S/ouba/hfn4+DpOXXL49u9TB3fbNn/2f/s4nwcTJ6DtmybqeOb7dr+F+fjUFD/D+a131sKOwsXLtTbb7+tnTt3KiQkRGFhYSpZsqTS0tL0/fffy8/PTz179tTw4cMVGRnpVuG3Q2JiosaPH5+rffXq1fL39/foWDU82lvRVf1UstklmGbFjyYXENXM5AIKhw11G5tdgnlWrDB1+EgV3vPB7RSxPcLsEkyzQgUzBzMyMvK0XZ7DTnR0tEqUKKHevXtr8eLFCg8Pd1mfmZmpLVu26MMPP1TDhg01e/ZsPfroo7dW9e+EhoZKkk6fPq1KlSo520+fPq0GDRo4t0lNTXV5XXZ2ttLS0pyvv5GRI0cqISHBuZyenq7w8HC1a9dOAQEBbtd8I2/tPePR/ooauyNb1U8l63BYjBx2Ux8RM83QqGBTx8+aNMrU8c2WbffShrqN1WbfVnk7cswuxxQ+I141dfw55+aYOr7ZbNk2RWyP0Il7TsjwNswuxxTPBT5XIP1euzPzZ/J89pk0aZLi4uJuut7X11etW7dW69at9eqrr+rYsWN57fqGqlSpotDQUK1bt84ZbtLT07Vt2zY999xvB61JkyY6d+6ckpOTFRMTI0lav369HA6HGjVq9Ie1+vr65mr38fGRj49Pvuq+XnE9wV/PYfcutsfC03PqlhXTE/z1vB058immx8LsOVhcT/DXM7yNYnssCmoO5rXfPJ99/ijoXC84OFjBwX/+2+zFixd15MgR5/LRo0e1Z88eBQUFKSIiQkOGDNHEiRNVvXp1ValSRaNHj1ZYWJg6d+4sSapVq5bat2+vfv36ae7cucrKylJ8fLx69OjBO7EAAIAkNx9Q3rVrl3x8fFSvXj1J0qeffqr58+erdu3aGjdunEqUKJGnfnbu3Kk2bdo4l6/dWurVq5cWLFigv/3tb7p06ZL69++vc+fOqXnz5kpKSnJ5CHrhwoWKj49X27ZtZbfb1bVrV02fPt2d3QIAABbkVth55plnNGLECNWrV08//PCDevTooS5duujjjz9WRkaGpk2blqd+WrduLcO4+SU9m82mCRMmaMKECTfdJigoSIsWLbrVXQAAAMWEW5+z89133zmfo/n444/VsmVLLVq0SAsWLNDixYs9WR8AAEC+uBV2DMOQw+GQJK1du1YdO3aUJIWHh+vXX3/1XHUAAAD55FbYadiwoSZOnKh//OMf2rRpkzp16iTptweMr/8QQAAAADO5FXamTZumXbt2KT4+XqNGjVK1atUkSZ988omaNm3q0QIBAADyw60HlKOiovTNN9/kan/99dfl5VWMP5IdAAAUOh79lDdPfS8WAACAp+Q57JQrV042W96+xCwtLc3tggAAADwpz2Hn95+dc+bMGU2cOFFxcXFq0qSJJGnLli1atWqVRo8e7fEiAQAA3JXnsNOrVy/n37t27aoJEyYoPj7e2fb8889r5syZWrt2rYYOHerZKgEAANzk1ruxVq1apfbt2+dqb9++vdauXZvvogAAADzFrbATHBysTz/9NFf7p59+mqcvAAUAALhd3Ho31vjx4/X0009r48aNatSokSRp27ZtSkpK0rvvvuvRAgEAAPLDrbDTu3dv1apVS9OnT9eSJUskSbVq1dKXX37pDD8AAACFgdufs9OoUSMtXLjQk7UAAAB4nNthx+Fw6MiRI0pNTXV+Keg1LVu2zHdhAAAAnuBW2Nm6dasee+wxHT9+XIZhuKyz2WzKycnxSHEAAAD55VbYefbZZ9WwYUN9/vnnqlSpUp4/WRkAAOB2cyvsHD58WJ988onz284BAAAKK7c+Z6dRo0Y6cuSIp2sBAADwOLeu7AwaNEgvvPCCUlJSVK9ePfn4+Lisj4qK8khxAAAA+eVW2Onatask6amnnnK22Ww2GYbBA8oAAKBQcSvsHD161NN1AAAAFAi3wk5kZKSn6wAAACgQbn+o4Pfff69p06bpwIEDkqTatWtr8ODBuuuuuzxWHAAAQH659W6sVatWqXbt2tq+fbuioqIUFRWlbdu2qU6dOlqzZo2nawQAAHCbW1d2RowYoaFDh2rSpEm52ocPH6777rvPI8UBAADkl1tXdg4cOKC+ffvman/qqaf07bff5rsoAAAAT3Er7FSoUEF79uzJ1b5nzx5VrFgxvzUBAAB4jFu3sfr166f+/fvrhx9+UNOmTSVJX331lSZPnqyEhASPFggAAJAfboWd0aNHq0yZMnrzzTc1cuRISVJYWJjGjRun559/3qMFAgAA5IdbYcdms2no0KEaOnSoLly4IEkqU6aMRwsDAADwBLc/QTk7O1vVq1d3CTmHDx+Wj4+PKleu7Kn6AAAA8sWtB5R79+6tr7/+Olf7tm3b1Lt37/zWBAAA4DFuhZ3du3erWbNmudobN258w3dpAQAAmMWtsGOz2ZzP6vze+fPn+cZzAABQqLgVdlq2bKnExESXYJOTk6PExEQ1b97cY8UBAADkl1thZ/LkyVq/fr1q1KihPn36qE+fPqpRo4Y2b96s119/3WPF5eTkaPTo0apSpYpKliypu+66S6+88ooMw3BuYxiGxowZo0qVKqlkyZKKjY3V4cOHPVYDAAAo2twKO7Vr19bevXvVrVs3paam6sKFC3ryySd18OBB1a1b12PFTZ48WXPmzNHMmTN14MABTZ48WVOmTNGMGTOc20yZMkXTp0/X3LlztW3bNpUqVUpxcXG6cuWKx+oAAABFl1tvPZd++xDB1157zZO15PL111/roYceUqdOnSRJlStX1gcffKDt27dL+u2qzrRp0/Tyyy/roYcekiS9//77CgkJ0bJly9SjR48CrQ8AABR+bl3ZkaQvvvhCjz/+uJo2baqffvpJkvSPf/xDX375pceKa9q0qdatW6fvvvtOkvTvf/9bX375pTp06CDpt8/7SUlJUWxsrPM1ZcuWVaNGjbRlyxaP1QEAAIout67sLF68WE888YR69uypXbt2KTMzU9Jv78Z67bXXtGLFCo8UN2LECKWnp6tmzZry8vJSTk6OXn31VfXs2VOSlJKSIkkKCQlxeV1ISIhz3Y1kZmY6a5ak9PR0SVJWVpaysrI8Uvs1dke2R/sraq7tf3E+Dp6eU7c8vt3L1PHNlv2f/c8uzsfB5Dloy7aZOr7Zru1/cT4OBfX/YF77dSvsTJw4UXPnztWTTz6pDz/80NnerFkzTZw40Z0ub+j//b//p4ULF2rRokWqU6eO9uzZoyFDhigsLEy9evVyu9/ExESNHz8+V/vq1avl7++fn5JzqeHR3oqu6qeSzS7BNCt+NLmAqNyfiVUcbajb2OwSzOOhX0DdFalIU8cvLCK2R5hdgmlWqGDmYEZGRp62cyvsHDp0SC1btszVXrZsWZ07d86dLm/oxRdf1IgRI5zP3tSrV0/Hjx9XYmKievXqpdDQUEnS6dOnValSJefrTp8+rQYNGty035EjR7p8O3t6errCw8PVrl07BQQEeKx+SXpr7xmP9lfU2B3Zqn4qWYfDYuSwu/2IWJE2NCrY1PGzJo0ydXyzZdu9tKFuY7XZt1XejuL5OWA+I141dfw55+aYOr7ZbNk2RWyP0Il7TsjwNv78BRb0XOBzBdLvtTszf8ats09oaKiOHDmS6zuwvvzyS1WtWtWdLm8oIyNDdrvrY0VeXl5yOBySpCpVqig0NFTr1q1zhpv09HRt27ZNzz138wPr6+srX1/fXO0+Pj7y8fHxWP2Siu0J/noOu3exPRaenlO3rJie4K/n7ciRTzE9FmbPweJ6gr+e4W0U22NRUHMwr/26dfbp16+fBg8erP/93/+VzWbTqVOntGXLFg0bNkyjR492p8sbeuCBB/Tqq68qIiJCderU0e7duzV16lQ99dRTkn77JOchQ4Zo4sSJql69uqpUqaLRo0crLCxMnTt39lgdAACg6HIr7IwYMUIOh0Nt27ZVRkaGWrZsKV9fXw0bNkyDBg3yWHEzZszQ6NGjNWDAAKWmpiosLEzPPPOMxowZ49zmb3/7my5duqT+/fvr3Llzat68uZKSkuTn5+exOgAAQNHlVtix2WwaNWqUXnzxRR05ckQXL15U7dq1Vbp0aY8WV6ZMGU2bNk3Tpk37w1omTJigCRMmeHRsAABgDW5/zo4klShRQrVr11bNmjW1du1aHThwwFN1AQAAeIRbYadbt26aOXOmJOny5cv6y1/+om7duikqKkqLFy/2aIEAAAD54VbY2bx5s1q0aCFJWrp0qRwOh86dO6fp06d79HN2AAAA8sutsHP+/HkFBQVJkpKSktS1a1f5+/urU6dOfOM4AAAoVNwKO+Hh4dqyZYsuXbqkpKQktWvXTpJ09uxZ3gUFAAAKFbfejTVkyBD17NlTpUuXVmRkpFq3bi3pt9tb9erV82R9AAAA+eJW2BkwYIAaNWqkEydO6L777nN+ynHVqlV5ZgcAABQqbn9+f0xMjGJiYlzaOnXqlO+CAAAAPCnPz+xMmjRJly9fztO227Zt0+eff+52UQAAAJ6S57Dz7bffKiIiQgMGDNDKlSv1yy+/ONdlZ2dr7969mj17tpo2baru3burTJkyBVIwAADArcjzbaz3339f//73vzVz5kw99thjSk9Pl5eXl3x9fZWRkSFJio6O1tNPP63evXvzriwAAFAo3NIzO/Xr19e7776rd955R3v37tXx48d1+fJllS9fXg0aNFD58uULqk4AAAC3uPWAst1uV4MGDdSgQQMPlwMAAOBZ+foiUAAAgMKOsAMAACyNsAMAACyNsAMAACwtX2HnyJEjWrVqlfPDBg3D8EhRAAAAnuJW2Dlz5oxiY2N19913q2PHjvr5558lSX379tULL7zg0QIBAADyw62wM3ToUHl7e+vEiRPy9/d3tnfv3l1JSUkeKw4AACC/3PqcndWrV2vVqlW68847XdqrV6+u48ePe6QwAAAAT3Drys6lS5dcruhck5aWJl9f33wXBQAA4CluhZ0WLVro/fffdy7bbDY5HA5NmTJFbdq08VhxAAAA+eXWbawpU6aobdu22rlzp65evaq//e1v2r9/v9LS0vTVV195ukYAAAC3uXVlp27duvruu+/UvHlzPfTQQ7p06ZIefvhh7d69W3fddZenawQAAHCbW1d2JKls2bIaNWqUJ2sBAADwOLfDzpUrV7R3716lpqbK4XC4rHvwwQfzXRgAAIAnuBV2kpKS9OSTT+rXX3/Ntc5msyknJyffhQEAAHiCW8/sDBo0SI8++qh+/vlnORwOlz8EHQAAUJi4FXZOnz6thIQEhYSEeLoeAAAAj3Ir7DzyyCPauHGjh0sBAADwPLee2Zk5c6YeffRRffHFF6pXr558fHxc1j///PMeKQ4AACC/3Ao7H3zwgVavXi0/Pz9t3LhRNpvNuc5msxF2AABAoeFW2Bk1apTGjx+vESNGyG53604YAADAbeFWUrl69aq6d+9O0AEAAIWeW2mlV69e+uijjzxdCwAAgMe5dRsrJydHU6ZM0apVqxQVFZXrAeWpU6d6pDhJ+umnnzR8+HCtXLlSGRkZqlatmubPn6+GDRtKkgzD0NixY/Xuu+/q3LlzatasmebMmaPq1at7rAYAAFB0uRV2vvnmG0VHR0uS9u3b57Lu9w8r59fZs2fVrFkztWnTRitXrlSFChV0+PBhlStXzrnNlClTNH36dP3f//2fqlSpotGjRysuLk7ffvut/Pz8PFYLAAAomtwKOxs2bPB0HTc0efJkhYeHa/78+c62KlWqOP9uGIamTZuml19+WQ899JAk6f3331dISIiWLVumHj163JY6AQBA4VWonzD+17/+pYYNG+rRRx9VxYoVFR0drXfffde5/ujRo0pJSVFsbKyzrWzZsmrUqJG2bNliRskAAKCQyfOVnYcfflgLFixQQECAHn744T/cdsmSJfkuTJJ++OEHzZkzRwkJCXrppZe0Y8cOPf/88ypRooR69eqllJQUScr1tRUhISHOdTeSmZmpzMxM53J6erokKSsrS1lZWR6p/Rq7I9uj/RU11/a/OB8HT8+pWx7f7mXq+GbL/s/+Zxfn42DyHLRle+7xhqLo2v4X5+NQUP8P5rXfPIedsmXLOp/HKVu2rHtV3SKHw6GGDRvqtddekyRFR0dr3759mjt3rnr16uV2v4mJiRo/fnyu9tWrV8vf39/tfm+khkd7K7qqn0o2uwTTrPjR5AKimplcQOGwoW5js0swz4oVpg4fqUhTxy8sIrZHmF2CaVaoYOZgRkZGnrbLc9iZP3++JkyYoGHDhrk8Q1OQKlWqpNq1a7u01apVS4sXL5YkhYaGSvrti0krVark3Ob06dNq0KDBTfsdOXKkEhISnMvp6ekKDw9Xu3btFBAQ4ME9kN7ae8aj/RU1dke2qp9K1uGwGDnsbj0iVuQNjQo2dfysSaNMHd9s2XYvbajbWG32bZW3I8fsckzhM+JVU8efc26OqeObzZZtU8T2CJ2454QMb8PsckzxXOBzBdLvtTszf+aWzj7jx4/Xs88+6/GrHzfTrFkzHTp0yKXtu+++U2Tkb78lVKlSRaGhoVq3bp0z3KSnp2vbtm167rmbH1hfX1/5+vrmavfx8cn1Nvr8Kq4n+Os57N7F9lh4ek7dsmJ6gr+etyNHPsX0WJg9B4vrCf56hrdRbI9FQc3BvPZ7S2cfw7i9P6ShQ4eqadOmeu2119StWzdt375d8+bN07x58yT99jb3IUOGaOLEiapevbrzredhYWHq3Lnzba0VAAAUTrf8q7YnP0fnz/zlL3/R0qVLNXLkSE2YMEFVqlTRtGnT1LNnT+c2f/vb33Tp0iX1799f586dU/PmzZWUlMRn7AAAAEluhJ277777TwNPWlqa2wVd7/7779f9999/0/U2m00TJkzQhAkTPDYmAACwjlsOO+PHj79t78YCAADIr1sOOz169FDFihULohYAAACPu6VPUL6dz+sAAAB4wi2Fndv9biwAAID8uqXbWA6Ho6DqAAAAKBCF+otAAQAA8ouwAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALK1IhZ1JkybJZrNpyJAhzrYrV65o4MCBCg4OVunSpdW1a1edPn3avCIBAEChUmTCzo4dO/TOO+8oKirKpX3o0KH67LPP9PHHH2vTpk06deqUHn74YZOqBAAAhU2RCDsXL15Uz5499e6776pcuXLO9vPnz+u9997T1KlTde+99yomJkbz58/X119/ra1bt5pYMQAAKCyKRNgZOHCgOnXqpNjYWJf25ORkZWVlubTXrFlTERER2rJly+0uEwAAFELeZhfwZz788EPt2rVLO3bsyLUuJSVFJUqUUGBgoEt7SEiIUlJSbtpnZmamMjMzncvp6emSpKysLGVlZXmm8P+wO7I92l9Rc23/i/Nx8PScuuXx7V6mjm+27P/sf3ZxPg4mz0Fbts3U8c12bf+L83EoqP8H89pvoQ47J0+e1ODBg7VmzRr5+fl5rN/ExESNHz8+V/vq1avl7+/vsXEkqYZHeyu6qp9KNrsE06z40eQCopqZXEDhsKFuY7NLMM+KFaYOH6lIU8cvLCK2R5hdgmlWqGDmYEZGRp62sxmGYRRIBR6wbNkydenSRV5e//2NLCcnRzabTXa7XatWrVJsbKzOnj3rcnUnMjJSQ4YM0dChQ2/Y742u7ISHh+vXX39VQECAR/fhrb1nPNpfUWN3ZKv6qWQdDouRw16os3WBGRoVbOr4WZNGmTq+2bLtXtpQt7Ha7Nsqb0eO2eWYwmfEq6aOP+fcHFPHN5st26aI7RE6cc8JGd6F9pRboJ4LfK5A+k1PT1f58uV1/vz5Pzx/F+qzT9u2bfXNN9+4tPXp00c1a9bU8OHDFR4eLh8fH61bt05du3aVJB06dEgnTpxQkyZNbtqvr6+vfH19c7X7+PjIx8fHo/tQXE/w13PYvYvtsfD0nLplxfQEfz1vR458iumxMHsOFtcT/PUMb6PYHouCmoN57bdQn33KlCmjunXrurSVKlVKwcHBzva+ffsqISFBQUFBCggI0KBBg9SkSRM1blyML1kDAACnQh128uKtt96S3W5X165dlZmZqbi4OM2ePdvssgAAQCFR5MLOxo0bXZb9/Pw0a9YszZo1y5yCAABAoVYkPmcHAADAXYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYU67CQmJuovf/mLypQpo4oVK6pz5846dOiQyzZXrlzRwIEDFRwcrNKlS6tr1646ffq0SRUDAIDCplCHnU2bNmngwIHaunWr1qxZo6ysLLVr106XLl1ybjN06FB99tln+vjjj7Vp0yadOnVKDz/8sIlVAwCAwsTb7AL+SFJSksvyggULVLFiRSUnJ6tly5Y6f/683nvvPS1atEj33nuvJGn+/PmqVauWtm7dqsaNG5tRNgAAKEQK9ZWd650/f16SFBQUJElKTk5WVlaWYmNjndvUrFlTERER2rJliyk1AgCAwqVQX9n5PYfDoSFDhqhZs2aqW7euJCklJUUlSpRQYGCgy7YhISFKSUm5aV+ZmZnKzMx0Lqenp0uSsrKylJWV5dG67Y5sj/ZX1Fzb/+J8HDw9p255fLuXqeObLfs/+59dnI+DyXPQlm0zdXyzXdv/4nwcCur/wbz2W2TCzsCBA7Vv3z59+eWX+e4rMTFR48ePz9W+evVq+fv757v/36vh0d6Kruqnks0uwTQrfjS5gKhmJhdQOGyoW4xva69YYerwkYo0dfzCImJ7hNklmGaFCmYOZmRk5Gm7IhF24uPjtXz5cm3evFl33nmnsz00NFRXr17VuXPnXK7unD59WqGhoTftb+TIkUpISHAup6enKzw8XO3atVNAQIBHa39r7xmP9lfU2B3Zqn4qWYfDYuSwF4np5nFDo4JNHT9r0ihTxzdbtt1LG+o2Vpt9W+XtyDG7HFP4jHjV1PHnnJtj6vhms2XbFLE9QifuOSHD2zC7HFM8F/hcgfR77c7MnynUZx/DMDRo0CAtXbpUGzduVJUqVVzWx8TEyMfHR+vWrVPXrl0lSYcOHdKJEyfUpEmTm/br6+srX1/fXO0+Pj7y8fHx6D4U1xP89Rx272J7LDw9p25ZMT3BX8/bkSOfYnoszJ6DxfUEfz3D2yi2x6Kg5mBe+y3UZ5+BAwdq0aJF+vTTT1WmTBnnczhly5ZVyZIlVbZsWfXt21cJCQkKCgpSQECABg0apCZNmvBOLAAAIKmQh505c3679Nm6dWuX9vnz56t3796SpLfeekt2u11du3ZVZmam4uLiNHv27NtcKQAAKKwKddgxjD+/3Ofn56dZs2Zp1qxZt6EiAABQ1BSpz9kBAAC4VYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaYQdAABgaZYJO7NmzVLlypXl5+enRo0aafv27WaXBAAACgFLhJ2PPvpICQkJGjt2rHbt2qX69esrLi5OqampZpcGAABMZomwM3XqVPXr1099+vRR7dq1NXfuXPn7++t///d/zS4NAACYrMiHnatXryo5OVmxsbHONrvdrtjYWG3ZssXEygAAQGHgbXYB+fXrr78qJydHISEhLu0hISE6ePDgDV+TmZmpzMxM5/L58+clSWlpacrKyvJofVfTz3q0v6LG7shWRkaGrqaflcNe5KebW86csZk6ftbVbFPHN1u23VBGRobSrmbL25Fjdjmm8DlzxtTxM89n/vlGVpYtZWRk6MrZKxY467rnjKNg5uCFCxckSYZh/OF2xfKwJyYmavz48bnaq1SpYkI1sLqxZhcAvDbd7ApQzA3X8ALt/8KFCypbtuxN1xf5sFO+fHl5eXnp9OnTLu2nT59WaGjoDV8zcuRIJSQkOJcdDofS0tIUHBwsm83c38KtJj09XeHh4Tp58qQCAgLMLgfFEHMQZmMOFhzDMHThwgWFhYX94XZFPuyUKFFCMTExWrdunTp37izpt/Cybt06xcfH3/A1vr6+8vX1dWkLDAws4EqLt4CAAP6Rw1TMQZiNOVgw/uiKzjVFPuxIUkJCgnr16qWGDRvqnnvu0bRp03Tp0iX16dPH7NIAAIDJLBF2unfvrl9++UVjxoxRSkqKGjRooKSkpFwPLQMAgOLHEmFHkuLj42962wrm8fX11dixY3PdNgRuF+YgzMYcNJ/N+LP3awEAABRhRf5DBQEAAP4IYQcAAFgaYQcAAFgaYQcec+zYMdlsNu3Zs8fsUmBxzDUAt4KwAwAALI2wAwAo9q5evWp2CZIKTx1WQ9jBTSUlJal58+YKDAxUcHCw7r//fn3//ffO9du3b1d0dLT8/PzUsGFD7d692+X1OTk56tu3r6pUqaKSJUuqRo0aevvtt1226d27tzp37qzXXntNISEhCgwM1IQJE5Sdna0XX3xRQUFBuvPOOzV//vzbss8wR2Gda4888ojL53cNGTJENptNBw8elPTbialUqVJau3ZtQRwWFKDWrVsrPj5eQ4YMUfny5eXr6yubzaZVq1YpOjpaJUuW1L333qvU1FStXLlStWrVUkBAgB577DFlZGQ4+/nkk09Ur149lSxZUsHBwYqNjdWlS5ck/XfOjR8/XhUqVFBAQICeffZZl0BzfR1xcXGSpE2bNumee+6Rr6+vKlWqpBEjRig7OzvX6+Lj41W2bFmVL19eo0eP/tNv/y62DOAmPvnkE2Px4sXG4cOHjd27dxsPPPCAUa9ePSMnJ8e4cOGCUaFCBeOxxx4z9u3bZ3z22WdG1apVDUnG7t27DcMwjKtXrxpjxowxduzYYfzwww/GP//5T8Pf39/46KOPnGP06tXLKFOmjDFw4EDj4MGDxnvvvWdIMuLi4oxXX33V+O6774xXXnnF8PHxMU6ePGnSkUBBK6xzbfr06UadOnWcfTRo0MAoX768MWfOHMMwDOPLL780fHx8jEuXLt2+gwWPaNWqlVG6dGnjxRdfNA4ePGjMnTvXkGQ0btzY+PLLL41du3YZ1apVM1q1amW0a9fO2LVrl7F582YjODjYmDRpkmEYhnHq1CnD29vbmDp1qnH06FFj7969xqxZs4wLFy4YhvHbnCtdurTRvXt3Y9++fcby5cuNChUqGC+99NJN6zh48KDx448/Gv7+/saAAQOMAwcOGEuXLjXKly9vjB07NtfrBg8ebBw8eNA55+fNm3dbj2NRQdhBnv3yyy+GJOObb74x3nnnHSM4ONi4fPmyc/2cOXNcTkA3MnDgQKNr167O5V69ehmRkZFGTk6Os61GjRpGixYtnMvZ2dlGqVKljA8++MCzO4RCq7DMtb179xo2m81ITU010tLSjBIlShivvPKK0b17d8MwDGPixIlG06ZNPbXbuI1atWplREdHO5c3bNhgSDLWrl3rbEtMTDQkGd9//72z7ZlnnjHi4uIMwzCM5ORkQ5Jx7NixG47Rq1cvIygoyCUMz5kzxyhdurRzHl5fh2EYxksvvWTUqFHDcDgczrZZs2blel2tWrVcthk+fLhRq1atWz4WxQG3sXBThw8f1l//+ldVrVpVAQEBqly5siTpxIkTOnDggKKiouTn5+fcvkmTJrn6mDVrlmJiYlShQgWVLl1a8+bN04kTJ1y2qVOnjuz2/07FkJAQ1atXz7ns5eWl4OBgpaamengPUVgU1rlWt25dBQUFadOmTfriiy8UHR2t+++/X5s2bZL0262G1q1be+ow4DaLiYnJ1RYVFeX8e0hIiPz9/VW1alWXtmvzo379+mrbtq3q1aunRx99VO+++67Onj3r0l/9+vXl7+/vXG7SpIkuXryokydP3rSOAwcOqEmTJrLZbM62Zs2a6eLFi/rxxx+dbY0bN3bZpkmTJjp8+LBycnLyfAyKC8IObuqBBx5QWlqa3n33XW3btk3btm2TlPcH6D788EMNGzZMffv21erVq7Vnzx716dMn1+t9fHxclm022w3bHA5HPvYGhVlhnWs2m00tW7bUxo0bncEmKipKmZmZ2rdvn77++mu1atXK3d2GyUqVKpWr7ffz4c/mh5eXl9asWaOVK1eqdu3amjFjhmrUqKGjR4/muw54FmEHN3TmzBkdOnRIL7/8stq2batatWq5/MZSq1Yt7d27V1euXHG2bd261aWPr776Sk2bNtWAAQMUHR2tatWquTx0CkiFf661atVKGzdu1MaNG9W6dWvZ7Xa1bNlSr7/+ujIzM9WsWTOPjIOiyWazqVmzZho/frx2796tEiVKaOnSpc71//73v3X58mXn8tatW1W6dGmFh4fftM9atWppy5YtLg8bf/XVVypTpozuvPNOZ9u1Xwp+33f16tXl5eXliV2zFMIObqhcuXIKDg7WvHnzdOTIEa1fv14JCQnO9Y899phsNpv69eunb7/9VitWrNAbb7zh0kf16tW1c+dOrVq1St99951Gjx6tHTt23O5dQSFX2Oda69at9e2332r//v1q3ry5s23hwoVq2LAhv5UXY9u2bdNrr72mnTt36sSJE1qyZIl++eUX1apVy7nN1atX1bdvX+fcHTt2rOLj411up15vwIABOnnypAYNGqSDBw/q008/1dixY5WQkODyuhMnTighIUGHDh3SBx98oBkzZmjw4MEFus9FFWEHN2S32/Xhhx8qOTlZdevW1dChQ/X6668715cuXVqfffaZvvnmG0VHR2vUqFGaPHmySx/PPPOMHn74YXXv3l2NGjXSmTNnNGDAgNu9KyjkCvtcq1evngIDA9WgQQOVLl1a0m9hJycnh+d1irmAgABt3rxZHTt21N13362XX35Zb775pjp06ODcpm3btqpevbpatmyp7t2768EHH9S4ceP+sN877rhDK1as0Pbt21W/fn09++yz6tu3r15++WWX7Z588kldvnxZ99xzjwYOHKjBgwerf//+BbGrRZ7NMHhTPgAAnta7d2+dO3dOy5Yt83jfrVu3VoMGDTRt2jSP921FXNkBAACWRtgBAACWxm0sAABgaVzZAQAAlkbYAQAAlkbYAQAAlkbYAQAAlkbYAVCkjRs3Tg0aNMhXH8eOHZPNZtOePXs8UhOAwoWwA6DAnTx5Uk899ZTCwsJUokQJRUZGavDgwTpz5swt9WOz2XJ9QNuwYcO0bt26fNUXHh6un3/+WXXr1s1XPwAKJ8IOgAL1ww8/qGHDhjp8+LA++OADHTlyRHPnztW6devUpEkTpaWl5av/0qVLKzg4OF99eHl5KTQ0VN7e3vnq52ZycnKc35QN4PYj7AAoUAMHDlSJEiW0evVqtWrVShEREerQoYPWrl2rn376SaNGjZIkVa5cWa+88or++te/qlSpUrrjjjs0a9YsZz+VK1eWJHXp0kU2m825fP1trN69e6tz58567bXXFBISosDAQE2YMEHZ2dl68cUXFRQUpDvvvFPz5893vub621i9e/eWzWbL9Wfjxo2SpMzMTA0bNkx33HGHSpUqpUaNGjnXSdKCBQsUGBiof/3rX6pdu7Z8fX114sQJjx9bAHlD2AFQYNLS0rRq1SoNGDBAJUuWdFkXGhqqnj176qOPPtK1zzZ9/fXXVb9+fe3evVsjRozQ4MGDtWbNGklyfov5/Pnz9fPPP//ht5qvX79ep06d0ubNmzV16lSNHTtW999/v8qVK6dt27bp2Wef1TPPPKMff/zxhq9/++239fPPPzv/DB48WBUrVlTNmjUlSfHx8dqyZYs+/PBD7d27V48++qjat2+vw4cPO/vIyMjQ5MmT9fe//1379+9XxYoV3T+QAPLHAIACsnXrVkOSsXTp0huunzp1qiHJOH36tBEZGWm0b9/eZX337t2NDh06OJdv1NfYsWON+vXrO5d79eplREZGGjk5Oc62GjVqGC1atHAuZ2dnG6VKlTI++OADwzAM4+jRo4YkY/fu3blqXLx4seHn52d8+eWXhmEYxvHjxw0vLy/jp59+ctmubdu2xsiRIw3DMIz58+cbkow9e/bc+MAAuK0K5gY1APyOkcdvpWnSpEmuZXe+1blOnTqy2/974TokJMTl4WMvLy8FBwcrNTX1D/vZvXu3nnjiCc2cOVPNmjWTJH3zzTfKycnR3Xff7bJtZmamy7NDJUqUUFRU1C3XDsDzCDsACky1atVks9l04MABdenSJdf6AwcOqFy5cqpQoYJHx/Xx8XFZttlsN2z7o4eGU1JS9OCDD+rpp59W3759ne0XL16Ul5eXkpOT5eXl5fKa0qVLO/9esmRJ2Wy2/OwGAA/hmR0ABSY4OFj33XefZs+ercuXL7usS0lJ0cKFC9W9e3dnKNi6davLNlu3blWtWrWcyz4+PsrJySnwuq9cuaKHHnpINWvW1NSpU13WRUdHKycnR6mpqapWrZrLn9DQ0AKvDcCtI+wAKFAzZ85UZmam4uLitHnzZp08eVJJSUm67777dMcdd+jVV191bvvVV19pypQp+u677zRr1ix9/PHHGjx4sHN95cqVtW7dOqWkpOjs2bMFVvMzzzyjkydPavr06frll1+UkpKilJQUXb16VXfffbd69uypJ598UkuWLNHRo0e1fft2JSYm6vPPPy+wmgC4j7ADoEBVr15dO3fuVNWqVdWtWzfddddd6t+/v9q0aaMtW7YoKCjIue0LL7ygnTt3Kjo6WhMnTtTUqVMVFxfnXP/mm29qzZo1Cg8PV3R0dIHVvGnTJv3888+qXbu2KlWq5Pzz9ddfS/rtHWFPPvmkXnjhBdWoUUOdO3fWjh07FBERUWA1AXCfzcjrk4MAUIAqV66sIUOGaMiQIWaXAsBiuLIDAAAsjbADAAAsjdtYAADA0riyAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALI2wAwAALO3/A7YHVLcf+7rGAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.bar(best_results['Optimizer'], best_results['Training Time'], color=bar_colors)\n",
        "# Display the graphs\n",
        "plt.xlabel('Optimizer')\n",
        "plt.ylabel('Time (seconds)')\n",
        "plt.title('Trainning Time comparison')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cudaEnv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
